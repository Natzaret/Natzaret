{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 3: Unsupervised learning with PCA, t-SNE, k-means, AHC and SOM\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsupervised learning techniques must be applied on two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A3-data.txt\n",
    "    # Features: 4 variables, 1 class\n",
    "    # Patterns: 360 patterns\n",
    "data = pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A3/A3-data.txt', sep=',', header=None)\n",
    "header_vector_data = data.iloc[0, :].tolist() #header\n",
    "df_data = pd.DataFrame(data.iloc[1:, :-1])\n",
    "class_data = pd.DataFrame(data.iloc[1:, -1])\n",
    "\n",
    "# Second dataset: from \"https://www.openml.org/search?type=data&status=active&id=188\"\n",
    "    # Features: at least 6 variables, and a class attribute\n",
    "    # The class attribute must refer to, at least, 4 different classes\n",
    "    #  Patterns: at least 200 patterns\n",
    "data_Eucaliptus, meta = arff.loadarff('C:/Users/Gari/Desktop/Assignments_NEC/A3/dataset_194_eucalyptus.arff')\n",
    "# Convert numpy array to DataFrame\n",
    "# [736 rows x 20 columns], number of classes 5\n",
    "dataframe_Eucaliptus = pd.DataFrame(data_Eucaliptus)\n",
    "df_Eucaliptus = dataframe_Eucaliptus.iloc[:, :-1]\n",
    "class_eucaliptus = dataframe_Eucaliptus.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta) # Details of the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Abbrev  Rep                Locality          Map_Ref   Latitude  \\\n",
      "0    b'Cra'  1.0   b'Central_Hawkes_Bay'  b'N135_382/137'  b'39__38'   \n",
      "1    b'Cra'  1.0   b'Central_Hawkes_Bay'  b'N135_382/137'  b'39__38'   \n",
      "2    b'Cra'  1.0   b'Central_Hawkes_Bay'  b'N135_382/137'  b'39__38'   \n",
      "3    b'Cra'  1.0   b'Central_Hawkes_Bay'  b'N135_382/137'  b'39__38'   \n",
      "4    b'Cra'  1.0   b'Central_Hawkes_Bay'  b'N135_382/137'  b'39__38'   \n",
      "..      ...  ...                     ...              ...        ...   \n",
      "731  b'WSh'  1.0  b'Southern_Hawkes_Bay'  b'N151_922/226'  b'40__36'   \n",
      "732  b'WSh'  1.0  b'Southern_Hawkes_Bay'  b'N151_922/226'  b'40__36'   \n",
      "733  b'WSh'  1.0  b'Southern_Hawkes_Bay'  b'N151_922/226'  b'40__36'   \n",
      "734  b'WSh'  1.0  b'Southern_Hawkes_Bay'  b'N151_922/226'  b'40__36'   \n",
      "735  b'WSh'  1.0  b'Southern_Hawkes_Bay'  b'N151_922/226'  b'40__36'   \n",
      "\n",
      "     Altitude  Rainfall  Frosts    Year     Sp   PMCno    DBH     Ht  Surv  \\\n",
      "0       100.0     850.0    -2.0  1980.0  b'co'  1520.0  18.45   9.96  40.0   \n",
      "1       100.0     850.0    -2.0  1980.0  b'fr'  1487.0  13.15   9.65  90.0   \n",
      "2       100.0     850.0    -2.0  1980.0  b'ma'  1362.0  10.32   6.50  50.0   \n",
      "3       100.0     850.0    -2.0  1980.0  b'nd'  1596.0  14.80   9.48  70.0   \n",
      "4       100.0     850.0    -2.0  1980.0  b'ni'  2088.0  14.50  10.78  90.0   \n",
      "..        ...       ...     ...     ...    ...     ...    ...    ...   ...   \n",
      "731     100.0    1250.0    -2.0  1983.0  b'fa'  2548.0  41.63  12.64  28.0   \n",
      "732     100.0    1250.0    -2.0  1983.0  b'fr'  2552.0  33.35  10.61  33.0   \n",
      "733     100.0    1250.0    -2.0  1983.0  b'ni'  2568.0  28.21   9.47  94.0   \n",
      "734     100.0    1250.0    -2.0  1983.0  b'ob'  1522.0  27.36  11.49  67.0   \n",
      "735     100.0    1250.0    -2.0  1983.0  b're'     NaN  22.34   8.84  25.0   \n",
      "\n",
      "     Vig  Ins_res  Stem_Fm  Crown_Fm  Brnch_Fm     Utility  \n",
      "0    4.0      3.0      3.5       4.0       3.5     b'good'  \n",
      "1    4.5      4.0      3.5       3.5       3.0     b'best'  \n",
      "2    2.3      2.5      3.0       3.5       3.0      b'low'  \n",
      "3    3.7      3.0      3.3       4.0       3.5     b'good'  \n",
      "4    4.0      2.7      3.3       3.0       3.0     b'good'  \n",
      "..   ...      ...      ...       ...       ...         ...  \n",
      "731  4.2      3.2      2.3       1.9       1.7  b'average'  \n",
      "732  4.5      4.0      2.8       3.0       1.5     b'good'  \n",
      "733  4.6      3.0      2.0       1.8       1.2     b'good'  \n",
      "734  4.7      3.3      3.4       3.4       3.0     b'good'  \n",
      "735  3.8      3.5      3.0       3.5       3.0  b'average'  \n",
      "\n",
      "[736 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_Eucaliptus) # Details of the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Comparing unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform unsupervised learning of the two datasets using the following algorithms:\n",
    "PCA, t-SNE, k-means, AHC and SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_pca(data, n_components=2, labels):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Scatter plot of the first two principal components\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('PCA Projection')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Scree plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def apply_tsne(data, perplexity=30, labels):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "    tsne_result = tsne.fit_transform(data)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.title(f't-SNE Projection (Perplexity={perplexity})')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def apply_kmeans(data, labels, k):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Scatter plot of the data colored according to the classes they belong to\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('True Classes')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of the data colored according to the clusters obtained by K-means\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'K-means Clustering (k={k})')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Compare with the real class labels\n",
    "    if k == len(np.unique(labels)):\n",
    "        cm = confusion_matrix(labels, cluster_labels)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHC\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Method UPGMA -> method average\n",
    "def apply_ahc(data, labels, method='average'):\n",
    "    # Calculate Euclidean distances between original patterns\n",
    "    distances = pdist(data)\n",
    "\n",
    "    # Calculate linkage matrix\n",
    "    linkage_matrix = linkage(distances, method=method)\n",
    "\n",
    "    # Plot dendrogram with colors representing original classes\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(linkage_matrix, \n",
    "               color_threshold=0, \n",
    "               labels=labels,\n",
    "               leaf_font_size=10,\n",
    "               above_threshold_color='k')\n",
    "    plt.title(f'Agglomerative Hierarchical Clustering ({method.capitalize()} Linkage)')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM\n",
    "from sompy.sompy import SOMFactory\n",
    "\n",
    "def apply_som(data, mapsize=(10, 10), topology='rectangular', learning_rate=0.02, neighborhood_function='gaussian', labels):\n",
    "    # Ensure at least 100 neurons\n",
    "    if mapsize[0] * mapsize[1] < 100:\n",
    "        raise ValueError(\"Number of neurons is less than 100. Please choose a larger mapsize.\")\n",
    "\n",
    "    som = SOMFactory.build(data, mapsize=mapsize, topology=topology, normalization='var', initialization='pca')\n",
    "    som.train(n_job=1, verbose=False, train_rough_len=20, train_finetune_len=100, learning_rate=learning_rate,\n",
    "              neighborhood=neighborhood_function)\n",
    "\n",
    "    # Plot component planes\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    som.component_planes()\n",
    "    plt.suptitle('Component Planes')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot U-matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    som.view_umatrix(bestmatches=True, colormap='viridis', colorbar=True)\n",
    "    plt.title('U-matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot heatmap of the most represented class in each position\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    som.plot_heatmap()\n",
    "    plt.title('Heatmap of the Most Represented Class')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the best matching units (BMUs) for each data point\n",
    "    bmus = som.find_bmu(data)\n",
    "    \n",
    "    # Calculate the most represented class for each position\n",
    "    unique_classes = np.unique(labels)\n",
    "    most_represented_class = np.zeros(som.codebook.mapsize)\n",
    "    for i, j in np.ndindex(som.codebook.mapsize):\n",
    "        most_represented_class[i, j] = np.argmax(np.bincount(labels[bmus == (i, j)]))\n",
    "\n",
    "    # Plot heatmap of the most represented class in each position\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(most_represented_class, cmap='viridis')\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.title('Heatmap of the Most Represented Class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1: data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = class_data\n",
    "\n",
    "apply_pca(df_data, labels)\n",
    "apply_tsne(df_data, labels)\n",
    "apply_kmeans(df_data, labels, k=1)\n",
    "apply_ahc(df_data)\n",
    "apply_som(df_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2: eucaliptus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = class_eucaliptus\n",
    "\n",
    "apply_pca(df_Eucaliptus, labels)\n",
    "apply_tsne(df_Eucaliptus, labels)\n",
    "apply_kmeans(df_Eucaliptus, labels, k=5)\n",
    "apply_ahc(df_Eucaliptus)\n",
    "apply_som(df_Eucaliptus, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
