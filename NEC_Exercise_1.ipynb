{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 1: Prediction with Back-Propagation and Linear Regression\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "\n",
    "Student: Natzaret G√°lvez R√≠squez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the predictions on  three datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A1-turbine.txt\n",
    "    # 5 features: the first 4 are the input variables, the last one is the value to predict\n",
    "    # 451 patterns: use the first 85% for training and validation, and the remaining 15% for test\n",
    "df_turbine=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-turbine.txt', sep='\\t', header=None)\n",
    "header_vector_turbine = df_turbine.iloc[0, :].tolist() #header\n",
    "df_turbine=df_turbine.iloc[1:,:-1]\n",
    "df_turbine=pd.DataFrame(df_turbine)\n",
    "\n",
    "# Second dataset: File: A1-synthetic.txt\n",
    "    # 10 features: the first 9 are the input variables, the last one is the value to predict\n",
    "    # 1000 patterns: use the first 80% for training and validation, and the remaining 20% for test\n",
    "df_synthetic=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-synthetic.txt', sep='\\t', header=None)\n",
    "header_vector_synthetic = df_synthetic.iloc[0, :].tolist() #header\n",
    "df_synthetic=df_synthetic.iloc[1:,:]\n",
    "df_synthetic=pd.DataFrame(df_synthetic)\n",
    "\n",
    "# Third dataset: from \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "    # At least 6 features, one of them used for prediction\n",
    "    # The prediction variable must take real (float or double) values; it should not represent a categorical value (that would correspond to a classification task)\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Wine Quality dataset [6497 rows x 11 columns]\n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df_wineQuality = wine_quality.data.features \n",
    "y = wine_quality.data.targets #quality of wine, an integer\n",
    "  \n",
    "# metadata \n",
    "#print(wine_quality.metadata) \n",
    "# variable information \n",
    "#print(wine_quality.variables) \n",
    "\n",
    "header_vector_wineQuality = df_wineQuality.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns of wine Quality:\n",
      "6497\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of patterns of wine Quality:\")\n",
    "print(len(df_wineQuality)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "# As we can observe by the following header of the wine quality, alcohol level is the last feature\n",
    "# We will use it as the value to predict\n",
    "print(header_vector_wineQuality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Turbine dataset\n",
    "X_turbine = df_turbine.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y_turbine = df_turbine.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "scaler_turbine = MinMaxScaler()\n",
    "X_turbine_normalized = scaler_turbine.fit_transform(X_turbine)\n",
    "#y_turbine_normalized = scaler_turbine.fit_transform(y_turbine.values.reshape(-1, 1))\n",
    "# Because the prediction column has all NaN values, it is not necessary to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Synthetic dataset\n",
    "X_synthetic = df_synthetic.iloc[:, :-1]\n",
    "y_synthetic = df_synthetic.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_synthetic = MinMaxScaler()\n",
    "X_synthetic_normalized = scaler_synthetic.fit_transform(X_synthetic)\n",
    "y_synthetic_normalized = scaler_synthetic.fit_transform(y_synthetic.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Wine Quality dataset: 0\n"
     ]
    }
   ],
   "source": [
    "##Wine Quality dataset\n",
    "#By the owners we know that this dataset does not have missing values, we can check by:\n",
    "missing_values_count = df_wineQuality.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "# No categorical variables in this dataset\n",
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "    return data_copy\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_wineQuality\n",
    "df_wineQuality_no_outliers = handle_outliers_iqr(df_wineQuality)\n",
    "\n",
    "# Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality_no_outliers.sample(frac=1, random_state=42)\n",
    "\n",
    "X_wineQuality = df_wineQuality_shuffled.iloc[:, :-1]\n",
    "y_wineQuality = df_wineQuality_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_wineQuality = MinMaxScaler()\n",
    "X_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(X_wineQuality)\n",
    "y_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(y_wineQuality.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 85% for training\n",
    "# Extract the remaining 15% for testing\n",
    "# Splitting Turbine dataset\n",
    "X_train_turbine, X_test_turbine, y_train_turbine, y_test_turbine = train_test_split(\n",
    "    X_turbine_normalized, y_turbine, test_size=0.15, shuffle=False\n",
    ")\n",
    "\n",
    "#Second dataset, synthetic\n",
    "X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
    "    X_synthetic_normalized, y_synthetic_normalized, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "X_train_wineQuality, X_test_wineQuality, y_train_wineQuality, y_test_wineQuality = train_test_split(\n",
    "    X_wineQuality_normalized_no_outliers,\n",
    "    y_wineQuality_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "#print(\"Total data size:\", len(df_wineQuality))\n",
    "#print(\"Training data size:\", len(df_wineQualityTrainingValidation))\n",
    "#print(\"Test data size:\", len(df_wineQualityTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementation of BP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network with back propagation\n",
    "\n",
    "    We will implement all the necessary methods in order for the network to learn.\n",
    "We will based the implementation on the algorithm and equations in the following document [G]:\n",
    " -https://campusvirtual.urv.cat/mod/resource/view.php?id=3657931\n",
    "\n",
    "    Moreover, the class MyNeuralNetwork will provide three public functions that can be called externally:\n",
    "-fit (X, y): has 2 parameters: an array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and a vector y of size (n_samples), which holds the target \n",
    "values (class labels) for the training samples. This method allows us to train the network with this data\n",
    "\n",
    "-predict (X): has 1 parameter, an array X of size (n_samples, n_features) that contains the samples. This method returns a vector with the predicted values for all the input samples\n",
    "\n",
    "-loss_epochs: returns 2 arrays of size (n_epochs, 2) that contain the evolution of the training error and the validation error for each of the epochs of the system, so this information can be plotted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with back propagation\n",
    "\n",
    "# The class MyNeuralNetwork will receive all these parameters in the class constructor:\n",
    "# n_units: array indicating the number of units in each layer, so, the length of this array will be the number of layers\n",
    "# epochs: number of epochs\n",
    "# learning_rate\n",
    "# momentum\n",
    "# activation: the selected activation function (sigmoid, relu, linear, tanh)\n",
    "# validation_percentage: the percentage of data that should be used as the validation set\n",
    "\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self, n_units, epochs, learning_rate, momentum, activation, validation_percentage):\n",
    "        self.L = len(n_units) - 1 # L: number of layers\n",
    "        self.n = n_units # n: an array with the number of units in each layer (including the input and output layers)\n",
    "        \n",
    "        self.h = [0] * (self.L + 1) # h: an array of arrays for the fields (h)\n",
    "        self.xi = [0] * (self.L + 1) # xi: an array of arrays for the activations (Œæ)\n",
    "        self.w = [0] * (self.L + 1) # w: an array of matrices for the weights (w)\n",
    "        self.theta = [0] * (self.L + 1) # theta: an array of arrays for the thresholds (Œ∏)\n",
    "        self.delta = [0] * (self.L + 1) # delta: an array of arrays for the propagation of errors (Œî)\n",
    "        self.d_w = [0] * (self.L + 1) # d_w: an array of matrices for the changes of the weights (Œ¥w)\n",
    "        self.d_theta = [0] * (self.L + 1) # d_theta: an array of arrays for the changes of the weights (Œ¥Œ∏)\n",
    "        self.d_w_prev = [0] * (self.L + 1) # d_w_prev: an array of matrices for the previous changes of the weights, used for the momentum term (Œ¥w(prev))\n",
    "        self.d_theta_prev = [0] * (self.L + 1) # d_theta_prev: an array of arrays for the previous changes of the thresholds, used for the momentum term (Œ¥Œ∏(prev))\n",
    "        \n",
    "\n",
    "       # Initialize arrays of arrays and arrays of matrices\n",
    "        for i in range(0, self.L):\n",
    "            self.h[i] = np.zeros(n_units[i])  # Initialize fields (h) as arrays of zeros\n",
    "            self.xi[i] = np.zeros(n_units[i])  # Initialize activations (Œæ) as arrays of zeros\n",
    "            self.theta[i] = np.random.randn(n_units[i])  # Initialize thresholds (Œ∏) as arrays of zeros\n",
    "            self.delta[i] = np.zeros(n_units[i])  # Initialize propagation of errors (Œî) as arrays of zeros\n",
    "            self.d_theta[i] = np.zeros(n_units[i])  # Initialize changes of weights (Œ¥Œ∏) as arrays of zeros\n",
    "            self.d_theta_prev[i] = np.zeros(n_units[i]) # Initialize previous changes of thresholds (Œ¥Œ∏(prev)) as arrays of zeros\n",
    "\n",
    "            # Initialize weights (w) as matrices with appropriate dimensions\n",
    "            self.w[i+1] = np.random.randn(n_units[i+1], n_units[i])\n",
    "\n",
    "            # Initialize changes of weights (Œ¥w) and previous changes of weights (Œ¥w(prev)) as matrices with appropriate dimensions\n",
    "            self.d_w[i+1] = np.zeros((n_units[i+1], n_units[i]))\n",
    "            self.d_w_prev[i+1] = np.zeros((n_units[i+1], n_units[i]))\n",
    "\n",
    "\n",
    "        self.fact = activation # fact: the name of the activation function that it will be used. It can be one of these four: sigmoid, relu, linear, tanh.\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.validation_percentage = validation_percentage\n",
    "\n",
    "        self.training_errors = []\n",
    "        self.validation_errors = []\n",
    "\n",
    "    # Function fit (X, y): to train the network with the data given by the parameters\n",
    "    def fit(self, X, y):\n",
    "        # Split the data into training and validation sets\n",
    "        if self.validation_percentage > 0:\n",
    "            num_validation = int(self.validation_percentage * X.shape[0])\n",
    "            X_train, y_train = X[:-num_validation], y[:-num_validation]\n",
    "            X_val, y_val = X[-num_validation:], y[-num_validation:]\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Iterate over each training example\n",
    "            for pat in range(X_train.shape[0]):\n",
    "\n",
    "                # Feed-forward propagation from pattern X_train[] to obtain the output o(X_train[])\n",
    "                # Set the input layer activation\n",
    "                self.xi[1] = X_train[pat] #randomPattern_X_train\n",
    "                # Calculate and store the activations of the neurons in each layer\n",
    "                for l in range(2, self.L + 1):\n",
    "                    self.h[l] = np.dot(self.w[l], self.xi[l-1]) - self.theta[l]\n",
    "                    self.xi[l] = self.activation_function(self.h[l])\n",
    "                #outputFeedForward= self.xi[self.L]\n",
    "\n",
    "                # Error backward propagation\n",
    "                # First, we compute a set of auxiliary variables\n",
    "                self.delta[self.L] = self.activation_function_derivative(self.xi[self.L]) * (self.xi[self.L] - y_train[pat])\n",
    "                # And then, we back propagate to the rest of the network\n",
    "                for l in range(self.L - 1, 1, -1): #for l in reversed(range(self.L + 1)):\n",
    "                    self.delta[l] = self.activation_function_derivative(self.xi[l]) * np.dot(self.delta[l+1], self.w[l+1])\n",
    "\n",
    "                # Update weights and thresholds\n",
    "                # In case that the prediction coincides with the desired output no modification of weights and thresholds is necessary\n",
    "                # delta[l]=0\n",
    "                if self.delta != 0:\n",
    "                    for l in range(2, self.L + 1):\n",
    "                        self.d_w[l] = -self.learning_rate * np.outer(self.delta[l], self.xi[l-1]) + self.momentum*self.d_w_prev[l]\n",
    "                        #self.d_w[l] = -self.learning_rate * np.outer(self.delta[l], self.xi[l-1]) + self.momentum*self.d_w_prev[l]\n",
    "                        self.d_theta[l] = self.learning_rate * self.delta[l] + self.momentum*self.d_theta_prev[l]\n",
    "                        #self.d_theta[l] = self.learning_rate * self.delta[l] + self.momentum*self.d_theta_prev[l]\n",
    "                        self.w[l] += self.d_w[l]\n",
    "                        self.theta[l] += self.d_theta[l]\n",
    "\n",
    "            # Due to the fact that the function predict() is based on a feed-foward propagation, we can use it in here    \n",
    "            # Feed-forward propagation all training patterns\n",
    "            predictions_train = self.predict(X_train)\n",
    "            # Calculate the trainig error\n",
    "            error_train = np.mean((predictions_train - y_train) ** 2)\n",
    "            self.training_errors.append(error_train)\n",
    "            # Feed-forward propagation all validation patterns\n",
    "            if self.validation_percentage > 0:\n",
    "                predictions_val = self.predict(X_val)\n",
    "                # Calculat the training error\n",
    "                error_val = np.mean((predictions_val - y_val) ** 2)\n",
    "                self.validation_errors.append(error_val)\n",
    "\n",
    "    \n",
    "    # Function predict(X): returns a vector with the predicted values for all the input samples\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            self.xi[1] = X[i]\n",
    "            for l in range(2, self.L + 1):\n",
    "                self.h[l] = np.dot(self.w[l], self.xi[l-1]) + self.theta[l]\n",
    "                self.xi[l] = self.activation_function(self.h[l])\n",
    "            predictions.append(self.xi[self.L])\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    # Function that will return the evolution of the training error and the validation error for each of the epochs of the system\n",
    "    def loss_epochs(self):\n",
    "        return np.array(self.training_errors), np.array(self.validation_errors) # 2 arrays of size (n_epochs, 2) that contain the evolution of \n",
    "                                                                                # the training error and the validation error for each of the epochs of the system\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        if self.fact == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fact == 'linear':\n",
    "            return x\n",
    "        elif self.fact == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.fact == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function type.\")\n",
    "\n",
    "    def activation_function_derivative(self, x):\n",
    "        if self.fact == 'sigmoid':\n",
    "            return x * (1 - x)\n",
    "        elif self.fact == 'linear':\n",
    "            return 1\n",
    "        elif self.fact == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif self.fact == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function type.\")\n",
    "        \n",
    "    # We use cross validation to find parameters that give the lower value for the global error using the neural network build\n",
    "    def cross_validate(self, X, y, n_folds=4):\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "        validation_errors = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            self.fit(X_train, y_train)\n",
    "            predictions_val = self.predict(X_val)\n",
    "            error_val = np.mean((predictions_val - y_val) ** 2)\n",
    "            validation_errors.append(error_val)\n",
    "\n",
    "        global_expected_error = np.mean(validation_errors)\n",
    "        return global_expected_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Obtaining and comparing predictions using the three models (BP, BP-F, MLR-F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the quality of a prediction, we will use the mean absolute percentage error (MAPE), given by:\n",
    "ùê∏(%) = 100 ‚àëùúá|(ùë¶^ùúá ‚àí ùëß^ùúá)/ùëß^ùúá|\n",
    "\n",
    "Where the best way to visualize the results is with scatter plots of the prediction value ùë¶^ùúá\n",
    "compared with the real value z^ùúá. The closer the points are to the diagonal, the better the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Parameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First of all, before caculating the mape we will look for the parameters that will give us the lower error value using our neural network testing it with cross-validation for n folds 4\n",
    "\n",
    "We will use the second dataset (A1-synthetic) due to its variables for first trying to finde the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2: A1-synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "global_expected_error = model.cross_validate(X_train_synthetic, y_train_synthetic)\n",
    "print(\"Global expected error\")\n",
    "print(global_expected_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.01, momentum=0.4, activation='sigmoid', validation_percentage=0.20)\n",
    "global_expected_error = model.cross_validate(X_train_synthetic, y_train_synthetic)\n",
    "print(\"Global expected error\")\n",
    "print(global_expected_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(n_units=[3, 9, 9, 3], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "global_expected_error = model.cross_validate(X_train_synthetic, y_train_synthetic)\n",
    "print(\"Global expected error\")\n",
    "print(global_expected_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MyNeuralNetwork and train the model\n",
    "# We choose the parameters with the best results\n",
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "#training_error, validation_error = model.fit(X_train_synthetic, y_train_synthetic)\n",
    "model.fit(X_train_synthetic, y_train_synthetic)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test_synthetic)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error\n",
      "138.99226527751236 %\n"
     ]
    }
   ],
   "source": [
    "# We calculate the MAPE\n",
    "# We will calculate the average percentage error across all data points\n",
    "mape = 100 * np.mean(np.abs((predictions - y_test_synthetic)/y_test_synthetic))\n",
    "print(\"Mean absolute percentage error\")\n",
    "print(mape, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output values of the neural network and also the input values are all normalized with min-max scaler. In order to represent a plot of the prediction values vs real values we will denormalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC_Exercise_1.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # De-normalize the prediction values and real values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m denormalizedPredictionResults \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49minverse_transform(predictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m denormalizedRealResults \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39minverse_transform(y_test_synthetic)\n",
      "File \u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC\\projects\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:542\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    530\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m        Transformed data.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    544\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    545\u001b[0m         X, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy, dtype\u001b[39m=\u001b[39mFLOAT_DTYPES, force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n\u001b[0;32m    548\u001b[0m     X \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC\\projects\\Lib\\site-packages\\sklearn\\utils\\validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[0;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1461\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# # De-normalize the prediction values and real values\n",
    "scaler = MinMaxScaler()\n",
    "denormalizedPredictionResults = scaler.inverse_transform(predictions)\n",
    "denormalizedRealResults = scaler.inverse_transform(y_test_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the prediction values vs real values\n",
    "# Create scatter plot\n",
    "plt.scatter(denormalizedRealResults, denormalizedPredictionResults)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Prediction Values')\n",
    "plt.title('Scatter Plot: Prediction values vs Real values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of the training and validation error as a function of the number of epochs\n",
    "# Dataset 2: A1-synthetic\n",
    "training_errors, validation_errors = model.loss_epochs()\n",
    "plt.plot(training_errors, label='Training Error')\n",
    "plt.plot(validation_errors, label='Validation Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the other two datasets:\n",
    "-Dataset 1: A1-turbine\n",
    "-Dataset 3: wineQuality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1: A1-turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1-turbine\n",
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "global_expected_error = model.cross_validate(X_train_turbine, y_train_turbine)\n",
    "print(\"Global expected error\")\n",
    "print(global_expected_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MyNeuralNetwork and train the model\n",
    "# We choose the parameters with the best results\n",
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "#training_error, validation_error = model.fit(X_train_synthetic, y_train_synthetic)\n",
    "model.fit(X_train_turbine, y_train_turbine)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test_turbine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1-turbine\n",
    "\n",
    "# We calculate the MAPE\n",
    "# We will calculate the average percentage error across all data points\n",
    "mape = 100 * np.mean(np.abs((predictions - y_test_synthetic)/y_test_synthetic))\n",
    "print(\"Mean absolute percentage error\")\n",
    "print(mape, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1-turbine\n",
    "\n",
    "# De-normalize the prediction and real values\n",
    "scaler = MinMaxScaler()\n",
    "denormalizedPredictionResults = scaler.inverse_transform(predictions)\n",
    "denormalizedRealResults = scaler.inverse_transform(y_test_synthetic)\n",
    "# Scatter plot of the prediction value vs real value\n",
    "# Create scatter plot\n",
    "plt.scatter(denormalizedRealResults, denormalizedPredictionResults)\n",
    "# Add labels and title\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Prediction Values')\n",
    "plt.title('Scatter Plot: Prediction values vs Real values')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Evolution of the training and validation error as a function of the number of epochs\n",
    "training_errors, validation_errors = model.loss_epochs()\n",
    "plt.plot(training_errors, label='Training Error')\n",
    "plt.plot(validation_errors, label='Validation Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 3: wine Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wineQuality\n",
    "\n",
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "global_expected_error = model.cross_validate(X_train_turbine, y_train_turbine)\n",
    "print(\"Global expected error\")\n",
    "print(global_expected_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MyNeuralNetwork and train the model\n",
    "# We choose the parameters with the best results\n",
    "model = MyNeuralNetwork(n_units=[1, 9, 9, 1], epochs=1000, learning_rate=0.02, momentum=0.4, activation='sigmoid', validation_percentage=0.30)\n",
    "#training_error, validation_error = model.fit(X_train_synthetic, y_train_synthetic)\n",
    "model.fit(X_train_turbine, y_train_turbine)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test_turbine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wineQuality\n",
    "\n",
    "# We calculate the MAPE\n",
    "# We will calculate the average percentage error across all data points\n",
    "mape = 100 * np.mean(np.abs((predictions - y_test_synthetic)/y_test_synthetic))\n",
    "print(\"Mean absolute percentage error\")\n",
    "print(mape, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wineQuality\n",
    "\n",
    "# De-normalize the prediction and real values\n",
    "scaler = MinMaxScaler()\n",
    "denormalizedPredictionResults = scaler.inverse_transform(predictions)\n",
    "denormalizedRealResults = scaler.inverse_transform(y_test_synthetic)\n",
    "# Scatter plot of the prediction value vs real value\n",
    "# Create scatter plot\n",
    "plt.scatter(denormalizedRealResults, denormalizedPredictionResults)\n",
    "# Add labels and title\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Prediction Values')\n",
    "plt.title('Scatter Plot: Prediction values vs Real values')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Evolution of the training and validation error as a function of the number of epochs\n",
    "training_errors, validation_errors = model.loss_epochs()\n",
    "plt.plot(training_errors, label='Training Error')\n",
    "plt.plot(validation_errors, label='Validation Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.2: Model result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-linear regression model from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network model, which can be used from Tensorflow, Scikit-learn or any other python library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
