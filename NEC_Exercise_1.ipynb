{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "\n",
    "#Packages\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 1: Prediction with Back-Propagation and Linear Regression\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the predictions on  three datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We upload the datasets\n",
    "\n",
    "#First dataset: File: A1-turbine.txt\n",
    "    #5 features: the first 4 are the input variables, the last one is the value to predict\n",
    "    #451 patterns: use the first 85% for training and validation, and the remaining 15% for test\n",
    "df_turbine=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-turbine.txt', sep='\\t', header=None)\n",
    "header_vector_turbine = df_turbine.iloc[0, :].tolist() #header\n",
    "df_turbine=df_turbine.iloc[1:,:]\n",
    "df_turbine=pd.DataFrame(df_turbine)\n",
    "\n",
    "#Second dataset: File: A1-synthetic.txt\n",
    "    #10 features: the first 9 are the input variables, the last one is the value to predict\n",
    "    #1000 patterns: use the first 80% for training and validation, and the remaining 20% for test\n",
    "df_synthetic=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-synthetic.txt', sep='\\t', header=None)\n",
    "header_vector_synthetic = df_synthetic.iloc[0, :].tolist() #header\n",
    "df_synthetic=df_synthetic.iloc[1:,:]\n",
    "df_synthetic=pd.DataFrame(df_synthetic)\n",
    "\n",
    "#Third dataset: from \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "    #At least 6 features, one of them used for prediction\n",
    "    #The prediction variable must take real (float or double) values; it should not represent a categorical value (that would correspond to a classification task)\n",
    "    #At least 400 patterns\n",
    "    #Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Wine Quality dataset [6496 rows x 11 columns]\n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df_wineQuality = wine_quality.data.features \n",
    "y = wine_quality.data.targets #quality of wine, an integer\n",
    "  \n",
    "# metadata \n",
    "#print(wine_quality.metadata) \n",
    "# variable information \n",
    "#print(wine_quality.variables) \n",
    "\n",
    "header_vector_wineQuality = df_wineQuality.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "# As we can observe by the following header of the wine quality, alcohol level is the last feature\n",
    "# We will use it as the value to predict\n",
    "print(header_vector_wineQuality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values, we check for and handle any missing values in our datasets\n",
    "#Categorical values, if there are categorical variables, we encode them appropriately\n",
    "#Outliers, we identify and handle the outliers in the data\n",
    "#Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "# - Save the preprocessed data\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed\n",
    "# - Save the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Calculate the index to split at\n",
    "split_index1 = int(0.85 * len(df_turbine))\n",
    "# Extract the first 85% for training\n",
    "df_turbineTrainingValidation = df_turbine.iloc[:split_index1, :]\n",
    "# Extract the remaining 15% for testing\n",
    "df_turbineTesting = df_turbine.iloc[split_index1:, :]\n",
    "\n",
    "#Second dataset, synthetic\n",
    "split_index2 = int(0.80 * len(df_synthetic)) # split 80% for training and validation and 20% for testing\n",
    "df_syntheticTrainingValidation = df_synthetic.iloc[:split_index2, :]\n",
    "df_syntheticTesting = df_synthetic.iloc[split_index2:, :]\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "#Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality.sample\n",
    "# Calculate the split indices\n",
    "split_index3 = int(0.8 * len(df_wineQuality))\n",
    "# Split the data into training + validation and test sets\n",
    "df_wineQualityTrainingValidation = df_wineQuality[:split_index3]\n",
    "df_wineQualityTesting = df_wineQuality[split_index3:]\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "#print(\"Total data size:\", len(df_wineQuality))\n",
    "#print(\"Training data size:\", len(df_wineQualityTrainingValidation))\n",
    "#print(\"Test data size:\", len(df_wineQualityTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementation of BP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Obtaining and comparing predictions using the three models (BP, BP-F, MLR-F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Parameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.2: Model result comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
