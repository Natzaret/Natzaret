{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 1: Prediction with Back-Propagation and Linear Regression\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the predictions on  three datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A1-turbine.txt\n",
    "    # 5 features: the first 4 are the input variables, the last one is the value to predict\n",
    "    # 451 patterns: use the first 85% for training and validation, and the remaining 15% for test\n",
    "df_turbine=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-turbine.txt', sep='\\t', header=None)\n",
    "header_vector_turbine = df_turbine.iloc[0, :].tolist() #header\n",
    "df_turbine=df_turbine.iloc[1:,:]\n",
    "df_turbine=pd.DataFrame(df_turbine)\n",
    "\n",
    "# Second dataset: File: A1-synthetic.txt\n",
    "    # 10 features: the first 9 are the input variables, the last one is the value to predict\n",
    "    # 1000 patterns: use the first 80% for training and validation, and the remaining 20% for test\n",
    "df_synthetic=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-synthetic.txt', sep='\\t', header=None)\n",
    "header_vector_synthetic = df_synthetic.iloc[0, :].tolist() #header\n",
    "df_synthetic=df_synthetic.iloc[1:,:]\n",
    "df_synthetic=pd.DataFrame(df_synthetic)\n",
    "\n",
    "# Third dataset: from \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "    # At least 6 features, one of them used for prediction\n",
    "    # The prediction variable must take real (float or double) values; it should not represent a categorical value (that would correspond to a classification task)\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Wine Quality dataset [6496 rows x 11 columns]\n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df_wineQuality = wine_quality.data.features \n",
    "y = wine_quality.data.targets #quality of wine, an integer\n",
    "  \n",
    "# metadata \n",
    "#print(wine_quality.metadata) \n",
    "# variable information \n",
    "#print(wine_quality.variables) \n",
    "\n",
    "header_vector_wineQuality = df_wineQuality.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can observe by the following header of the wine quality, alcohol level is the last feature\n",
    "# We will use it as the value to predict\n",
    "print(header_vector_wineQuality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Turbine dataset\n",
    "X_turbine = df_turbine.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y_turbine = df_turbine.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "scaler_turbine = MinMaxScaler()\n",
    "X_turbine_normalized = scaler_turbine.fit_transform(X_turbine)\n",
    "#y_turbine_normalized = scaler_turbine.fit_transform(y_turbine.values.reshape(-1, 1))\n",
    "# Because the prediction column has all NaN values, it is not necessary to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Synthetic dataset\n",
    "X_synthetic = df_synthetic.iloc[:, :-1]\n",
    "y_synthetic = df_synthetic.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_synthetic = MinMaxScaler()\n",
    "X_synthetic_normalized = scaler_synthetic.fit_transform(X_synthetic)\n",
    "y_synthetic_normalized = scaler_synthetic.fit_transform(y_synthetic.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "#By the owners we know that this dataset has not missing values, we can check by:\n",
    "missing_values_count = df_wineQuality.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "# No categorical variables in this dataset\n",
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "    return data_copy\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_wineQuality\n",
    "df_wineQuality_no_outliers = handle_outliers_iqr(df_wineQuality)\n",
    "\n",
    "#Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality_no_outliers.sample(frac=1, random_state=42)\n",
    "\n",
    "X_wineQuality = df_wineQuality_shuffled.iloc[:, :-1]\n",
    "y_wineQuality = df_wineQuality_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_wineQuality = StandardScaler()\n",
    "X_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(X_wineQuality)\n",
    "y_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(y_wineQuality.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 85% for training\n",
    "# Extract the remaining 15% for testing\n",
    "# Splitting Turbine dataset\n",
    "X_train_turbine, X_test_turbine, y_train_turbine, y_test_turbine = train_test_split(\n",
    "    X_turbine_normalized, y_turbine, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "#Second dataset, synthetic\n",
    "X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
    "    X_synthetic_normalized, y_synthetic_normalized, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "X_train_wineQuality, X_test_wineQuality, y_train_wineQuality, y_test_wineQuality = train_test_split(\n",
    "    X_wineQuality_normalized_no_outliers,\n",
    "    y_wineQuality_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "#print(\"Total data size:\", len(df_wineQuality))\n",
    "#print(\"Training data size:\", len(df_wineQualityTrainingValidation))\n",
    "#print(\"Test data size:\", len(df_wineQualityTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementation of BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, n_units, epochs, learning_rate, momentum, activation, validation_percentage):\n",
    "        self.L = len(n_units) - 1\n",
    "        self.n = n_units\n",
    "        self.h = [None] * (self.L + 1)\n",
    "        self.xi = [None] * (self.L + 1)\n",
    "        self.w = [None] * (self.L + 1)\n",
    "        self.theta = [None] * (self.L + 1)\n",
    "        self.delta = [None] * (self.L + 1)\n",
    "        self.d_w = [None] * (self.L + 1)\n",
    "        self.d_theta = [None] * (self.L + 1)\n",
    "        self.d_w_prev = [None] * (self.L + 1)\n",
    "        self.d_theta_prev = [None] * (self.L + 1)\n",
    "        self.fact = activation\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.validation_percentage = validation_percentage\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Split the data into training and validation sets\n",
    "        if self.validation_percentage > 0:\n",
    "            num_validation = int(self.validation_percentage * X.shape[0])\n",
    "            X_train, y_train = X[:-num_validation], y[:-num_validation]\n",
    "            X_val, y_val = X[-num_validation:], y[-num_validation:]\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "\n",
    "        # Initialize weights and thresholds\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.w[l] = np.random.randn(self.n[l], self.n[l-1])\n",
    "            self.theta[l] = np.random.randn(self.n[l])\n",
    "            self.d_w_prev[l] = np.zeros_like(self.w[l])\n",
    "            self.d_theta_prev[l] = np.zeros_like(self.theta[l])\n",
    "\n",
    "        # Perform training for the specified number of epochs\n",
    "        training_error = []\n",
    "        validation_error = []\n",
    "        for epoch in range(self.epochs):\n",
    "            # Iterate over each training example\n",
    "            for i in range(X_train.shape[0]):\n",
    "                # Set the input layer activation\n",
    "                self.xi[1] = X_train[i]\n",
    "\n",
    "                # Feed-forward propagation\n",
    "                for l in range(2, self.L + 1):\n",
    "                    self.h[l] = np.dot(self.w[l], self.xi[l-1]) + self.theta[l]\n",
    "                    self.xi[l] = self.activation_function(self.h[l])\n",
    "\n",
    "                # Backward propagation\n",
    "                self.delta[self.L] = self.sigmoid_derivative(self.xi[self.L]) * (self.xi[self.L] - y_train[i])\n",
    "                for l in range(self.L - 1, 1, -1):\n",
    "                    self.delta[l] = self.sigmoid_derivative(self.xi[l]) * np.dot(self.delta[l+1], self.w[l+1])\n",
    "\n",
    "                # Update weights and thresholds\n",
    "                for l in range(2, self.L + 1):\n",
    "                    self.d_w[l] = -self.learning_rate * np.outer(self.delta[l], self.xi[l-1])\n",
    "                    self.d_theta[l] = -self.learning_rate * self.delta[l]\n",
    "                    self.w[l] += self.d_w[l] + self.momentum * self.d_w_prev[l]\n",
    "                    self.theta[l] += self.d_theta[l] + self.momentum * self.d_theta_prev[l]\n",
    "                    self.d_w_prev[l] = self.d_w[l]\n",
    "                    self.d_theta_prev[l] = self.d_theta[l]\n",
    "\n",
    "            # Calculate training error\n",
    "            predictions_train = self.predict(X_train)\n",
    "            error_train = np.mean((y_train - predictions_train) ** 2)\n",
    "            training_error.append(error_train)\n",
    "\n",
    "            # Calculate validation error if validation set is used\n",
    "            if self.validation_percentage > 0:\n",
    "                predictions_val = self.predict(X_val)\n",
    "                error_val = np.mean((y_val - predictions_val) ** 2)\n",
    "                validation_error.append(error_val)\n",
    "\n",
    "        return training_error, validation_error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            self.xi[1] = X[i]\n",
    "            for l in range(2, self.L + 1):\n",
    "                self.h[l] = np.dot(self.w[l], self.xi[l-1]) + self.theta[l]\n",
    "                self.xi[l] = self.activation_function(self.h[l])\n",
    "            predictions.append(self.xi[self.L])\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def loss_epochs(self):\n",
    "        return np.array(self.training_errors), np.array(self.validation_errors) # 2 arrays of size (n_epochs, 2) that contain the evolution of \n",
    "                                                                                # the training error and the validation error for each of the epochs of the system\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        if self.fact == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fact == 'linear':\n",
    "            return x\n",
    "        elif self.fact == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.fact == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function type.\")\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MyNeuralNetwork and train the model\n",
    "model = MyNeuralNetwork(n_units=[4, 9, 1], epochs=1000, learning_rate=0.1, momentum=0.9, activation='sigmoid', validation_percentage=0.2)\n",
    "training_error, validation_error = model.fit(X_train_synthetic, y_train_synthetic)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test_synthetic)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors, validation_errors = model.loss_epochs()\n",
    "plt.plot(training_errors, label='Training Error')\n",
    "plt.plot(validation_errors, label='Validation Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Obtaining and comparing predictions using the three models (BP, BP-F, MLR-F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Parameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.2: Model result comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
