{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 1: Prediction with Back-Propagation and Linear Regression\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the predictions on  three datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A1-turbine.txt\n",
    "    # 5 features: the first 4 are the input variables, the last one is the value to predict\n",
    "    # 451 patterns: use the first 85% for training and validation, and the remaining 15% for test\n",
    "df_turbine=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-turbine.txt', sep='\\t', header=None)\n",
    "header_vector_turbine = df_turbine.iloc[0, :].tolist() #header\n",
    "df_turbine=df_turbine.iloc[1:,:]\n",
    "df_turbine=pd.DataFrame(df_turbine)\n",
    "\n",
    "# Second dataset: File: A1-synthetic.txt\n",
    "    # 10 features: the first 9 are the input variables, the last one is the value to predict\n",
    "    # 1000 patterns: use the first 80% for training and validation, and the remaining 20% for test\n",
    "df_synthetic=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-synthetic.txt', sep='\\t', header=None)\n",
    "header_vector_synthetic = df_synthetic.iloc[0, :].tolist() #header\n",
    "df_synthetic=df_synthetic.iloc[1:,:]\n",
    "df_synthetic=pd.DataFrame(df_synthetic)\n",
    "\n",
    "# Third dataset: from \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "    # At least 6 features, one of them used for prediction\n",
    "    # The prediction variable must take real (float or double) values; it should not represent a categorical value (that would correspond to a classification task)\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Wine Quality dataset [6496 rows x 11 columns]\n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df_wineQuality = wine_quality.data.features \n",
    "y = wine_quality.data.targets #quality of wine, an integer\n",
    "  \n",
    "# metadata \n",
    "#print(wine_quality.metadata) \n",
    "# variable information \n",
    "#print(wine_quality.variables) \n",
    "\n",
    "header_vector_wineQuality = df_wineQuality.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "# As we can observe by the following header of the wine quality, alcohol level is the last feature\n",
    "# We will use it as the value to predict\n",
    "print(header_vector_wineQuality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Turbine dataset\n",
    "X_turbine = df_turbine.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y_turbine = df_turbine.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "scaler_turbine = MinMaxScaler()\n",
    "X_turbine_normalized = scaler_turbine.fit_transform(X_turbine)\n",
    "#y_turbine_normalized = scaler_turbine.fit_transform(y_turbine.values.reshape(-1, 1))\n",
    "# Because the prediction column has all NaN values, it is not necessary to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Synthetic dataset\n",
    "X_synthetic = df_synthetic.iloc[:, :-1]\n",
    "y_synthetic = df_synthetic.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_synthetic = MinMaxScaler()\n",
    "X_synthetic_normalized = scaler_synthetic.fit_transform(X_synthetic)\n",
    "y_synthetic_normalized = scaler_synthetic.fit_transform(y_synthetic.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Wine Quality dataset: 0\n"
     ]
    }
   ],
   "source": [
    "##Wine Quality dataset\n",
    "#By the owners we know that this dataset has not missing values, we can check by:\n",
    "missing_values_count = df_wineQuality.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "# No categorical variables in this dataset\n",
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "    return data_copy\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_wineQuality\n",
    "df_wineQuality_no_outliers = handle_outliers_iqr(df_wineQuality)\n",
    "\n",
    "#Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality_no_outliers.sample(frac=1, random_state=42)\n",
    "\n",
    "X_wineQuality = df_wineQuality_shuffled.iloc[:, :-1]\n",
    "y_wineQuality = df_wineQuality_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_wineQuality = StandardScaler()\n",
    "X_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(X_wineQuality)\n",
    "y_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(y_wineQuality.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 85% for training\n",
    "# Extract the remaining 15% for testing\n",
    "# Splitting Turbine dataset\n",
    "X_train_turbine, X_test_turbine, y_train_turbine, y_test_turbine = train_test_split(\n",
    "    X_turbine_normalized, y_turbine, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "#Second dataset, synthetic\n",
    "X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
    "    X_synthetic_normalized, y_synthetic_normalized, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "X_train_wineQuality, X_test_wineQuality, y_train_wineQuality, y_test_wineQuality = train_test_split(\n",
    "    X_wineQuality_normalized_no_outliers,\n",
    "    y_wineQuality_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "#print(\"Total data size:\", len(df_wineQuality))\n",
    "#print(\"Training data size:\", len(df_wineQualityTrainingValidation))\n",
    "#print(\"Test data size:\", len(df_wineQualityTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementation of BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, layers, epochs=100, learning_rate=0.01, momentum=0.9, fact='sigmoid', validation_split=0.2):\n",
    "        self.L = len(layers)    # L: number of layers\n",
    "        self.n = layers.copy()  # n: an array with the number of units in each layer (including the input and output layers) (number of neurons in each layer)\n",
    "        self.h = [np.zeros(l) for l in self.n[1:]] # h: an array of arrays for the fields (h)\n",
    "        self.xi = []            # xi: an array of arrays for the activations (ξ) (node values), an array of length as the number of layers (one component per layer) \n",
    "                                # and each position of the array of xi, for example xi[1], array of real numbers of length of the units in the first layer and so on\n",
    "        for lay in range(self.L):\n",
    "            self.xi.append(np.random.randn(layers[lay]))\n",
    "        self.w = []             # w: an array of matrices for the weights (w) (edge weights)\n",
    "                                # an array where w[1] is not used, and the array in w[2] will be (layer[2])x(layer[1]) and so on\n",
    "        self.w.append(np.zeros((1, 1)))\n",
    "        for lay in range(1, self.L):\n",
    "            self.w.append(np.random.randn(layers[lay], layers[lay - 1]))\n",
    "\n",
    "        self.theta = [np.zeros(l) for l in layers[1:]] # theta: an array of arrays for the thresholds (θ)\n",
    "\n",
    "        self.delta = [np.zeros(l) for l in layers[1:]] # delta: an array of arrays for the propagation of errors (Δ)\n",
    "        self.d_w = [np.zeros((layers[i], layers[i-1])) for i in range(1, self.L)] # d_w: an array of matrices for the changes of the weights (δw)\n",
    "        self.d_theta = [np.zeros(l) for l in layers[1:]] # d_theta: an array of arrays for the changes of the weights (δθ)\n",
    "        self.d_w_prev = [np.zeros((layers[i], layers[i-1])) for i in range(1, self.L)] # d_w_prev: an array of matrices for the previous changes of the weights, used for the momentum term (δw(prev))\n",
    "        self.d_theta_prev = [np.zeros(l) for l in layers[1:]] # d_theta_prev: an array of arrays for the previous changes of the thresholds, used for the momentum term (δθ(prev))\n",
    "        self.activation_function = fact # fact: the name of the activation function that it will be used. It can be one of these four: sigmoid, relu, linear, tanh.\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.training_errors = []\n",
    "        self.validation_errors = []\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        # Initialize validation data to None\n",
    "        self.validation_data = None\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return x\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return x * (1 - x)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        self.xi[0] = x\n",
    "        for layer in range(1, self.L):\n",
    "            # Use np.dot with proper dimensions\n",
    "            self.h[layer - 1] = np.dot(self.w[layer - 1], self.xi[layer - 1]) - self.theta[layer - 1]\n",
    "            self.xi[layer] = self.activation(self.h[layer-1])\n",
    "\n",
    "    def back_propagation(self, y):\n",
    "        self.delta[-1] = self.activation_derivative(self.xi[-1]) * (y - self.xi[-1])\n",
    "        for layer in range(self.L-2, 0, -1):\n",
    "            self.delta[layer-1] = self.activation_derivative(self.xi[layer]) * np.dot(self.w[layer].T, self.delta[layer])\n",
    "\n",
    "        for layer in range(self.L-1, 0, -1):\n",
    "            # Access weights using w[layer][i, j]\n",
    "            self.d_w[layer-1] = self.learning_rate * np.outer(self.delta[layer-1], self.xi[layer-1]) + self.momentum * self.d_w_prev[layer-1]\n",
    "            self.d_theta[layer-1] = -self.learning_rate * self.delta[layer-1] + self.momentum * self.d_theta_prev[layer-1]\n",
    "\n",
    "            self.w[layer-1] += self.d_w[layer-1]\n",
    "            self.theta[layer-1] += self.d_theta[layer-1]\n",
    "\n",
    "            self.d_w_prev[layer-1] = self.d_w[layer-1]\n",
    "            self.d_theta_prev[layer-1] = self.d_theta[layer-1]\n",
    "\n",
    "    def fit(self, X, y): # X of size (n_samples, n_features) which holds the training samples represented as floating point feature vectors; \n",
    "                         # and a vector y of size (n_samples), which holds the target values (class labels) for the training samples\n",
    "        # First, we split the data into training and validation sets\n",
    "        if self.validation_split > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.validation_split, random_state=42)\n",
    "            self.validation_data = (X_val, y_val)\n",
    "        else:\n",
    "            self.validation_data = None\n",
    "            X_train, y_train = X, y  # Use the full dataset for training\n",
    "        \n",
    "        # Implementation of fit function\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X_train)):\n",
    "                self.feed_forward(X_train[i])\n",
    "                self.back_propagation(y_train[i])\n",
    "\n",
    "        # Calculate training error\n",
    "        training_error = 0.5 * np.mean((y_train - self.predict(X_train)) ** 2)\n",
    "        self.training_errors.append(training_error)\n",
    "\n",
    "        # Calculate validation error if validation data is provided\n",
    "        if self.validation_data: # Checks if self.validation_data is not None or empty\n",
    "            X_val, y_val = self.validation_data\n",
    "            validation_error = 0.5 * np.mean((y_val - self.predict(X_val)) ** 2)\n",
    "            self.validation_errors.append(validation_error)\n",
    "        else : # Give output in case that there is no validation data\n",
    "            validation_error = None\n",
    "            self.validation_errors.append(validation_error)\n",
    "\n",
    "    def predict(self, X): # an array X of size (n_samples, n_features) that contains the samples\n",
    "        # Implementation of predict function\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            self.feed_forward(X[i])\n",
    "            predictions.append(self.xi[-1].copy())\n",
    "        return np.array(predictions) #  vector with the predicted values for all the input samples\n",
    "\n",
    "    def loss_epochs(self):\n",
    "        return np.array(self.training_errors), np.array(self.validation_errors) # 2 arrays of size (n_epochs, 2) that contain the evolution of \n",
    "                                                                                # the training error and the validation error for each of the epochs of the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1) and (9,) not aligned: 1 (dim 1) != 9 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC_Exercise_1.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m layers \u001b[39m=\u001b[39m [\u001b[39m10\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m neural_network \u001b[39m=\u001b[39m MyNeuralNetwork(layers\u001b[39m=\u001b[39mlayers, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, fact\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m, validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m neural_network\u001b[39m.\u001b[39;49mfit(X_train_synthetic, y_train_synthetic)\n",
      "\u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC_Exercise_1.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train)):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(X_train[i])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_propagation(y_train[i])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m# Calculate training error\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC_Exercise_1.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxi[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# Use np.dot with proper dimensions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh[layer \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw[layer \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxi[layer \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m]) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta[layer \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X32sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxi[layer] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh[layer\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,1) and (9,) not aligned: 1 (dim 1) != 9 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Example network with 3 layers: 10 input nodes, 9 hidden nodes, and 1 output node\n",
    "layers = [10, 9, 5, 1]\n",
    "neural_network = MyNeuralNetwork(layers=layers, epochs=100, learning_rate=0.01, momentum=0.9, fact='sigmoid', validation_split=0.2)\n",
    "neural_network.fit(X_train_synthetic, y_train_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors, validation_errors = neural_network.loss_epochs()\n",
    "plt.plot(training_errors, label='Training Error')\n",
    "plt.plot(validation_errors, label='Validation Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions_turbine_test = neural_network.predict(X_test_turbine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Obtaining and comparing predictions using the three models (BP, BP-F, MLR-F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Parameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.2: Model result comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
