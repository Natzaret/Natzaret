{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 1: Prediction with Back-Propagation and Linear Regression\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the predictions on  three datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A1-turbine.txt\n",
    "    # 5 features: the first 4 are the input variables, the last one is the value to predict\n",
    "    # 451 patterns: use the first 85% for training and validation, and the remaining 15% for test\n",
    "df_turbine=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-turbine.txt', sep='\\t', header=None)\n",
    "header_vector_turbine = df_turbine.iloc[0, :].tolist() #header\n",
    "df_turbine=df_turbine.iloc[1:,:]\n",
    "df_turbine=pd.DataFrame(df_turbine)\n",
    "\n",
    "# Second dataset: File: A1-synthetic.txt\n",
    "    # 10 features: the first 9 are the input variables, the last one is the value to predict\n",
    "    # 1000 patterns: use the first 80% for training and validation, and the remaining 20% for test\n",
    "df_synthetic=pd.read_csv('C:/Users/Gari/Desktop/NEC/A1-synthetic.txt', sep='\\t', header=None)\n",
    "header_vector_synthetic = df_synthetic.iloc[0, :].tolist() #header\n",
    "df_synthetic=df_synthetic.iloc[1:,:]\n",
    "df_synthetic=pd.DataFrame(df_synthetic)\n",
    "\n",
    "# Third dataset: from \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "    # At least 6 features, one of them used for prediction\n",
    "    # The prediction variable must take real (float or double) values; it should not represent a categorical value (that would correspond to a classification task)\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Wine Quality dataset [6496 rows x 11 columns]\n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "df_wineQuality = wine_quality.data.features \n",
    "y = wine_quality.data.targets #quality of wine, an integer\n",
    "  \n",
    "# metadata \n",
    "#print(wine_quality.metadata) \n",
    "# variable information \n",
    "#print(wine_quality.variables) \n",
    "\n",
    "header_vector_wineQuality = df_wineQuality.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "# As we can observe by the following header of the wine quality, alcohol level is the last feature\n",
    "# We will use it as the value to predict\n",
    "print(header_vector_wineQuality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"https://archive.ics.uci.edu/dataset/186/wine+quality\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Turbine dataset\n",
    "X_turbine = df_turbine.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y_turbine = df_turbine.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "scaler_turbine = MinMaxScaler()\n",
    "X_turbine_normalized = scaler_turbine.fit_transform(X_turbine)\n",
    "#y_turbine_normalized = scaler_turbine.fit_transform(y_turbine.values.reshape(-1, 1))\n",
    "# Because the prediction column has all NaN values, it is not necessary to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Synthetic dataset\n",
    "X_synthetic = df_synthetic.iloc[:, :-1]\n",
    "y_synthetic = df_synthetic.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_synthetic = MinMaxScaler()\n",
    "X_synthetic_normalized = scaler_synthetic.fit_transform(X_synthetic)\n",
    "y_synthetic_normalized = scaler_synthetic.fit_transform(y_synthetic.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Wine Quality dataset: 0\n"
     ]
    }
   ],
   "source": [
    "##Wine Quality dataset\n",
    "#By the owners we know that this dataset has not missing values, we can check by:\n",
    "missing_values_count = df_wineQuality.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "# No categorical variables in this dataset\n",
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "    return data_copy\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_wineQuality\n",
    "df_wineQuality_no_outliers = handle_outliers_iqr(df_wineQuality)\n",
    "\n",
    "#Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality_no_outliers.sample(frac=1, random_state=42)\n",
    "\n",
    "X_wineQuality = df_wineQuality_shuffled.iloc[:, :-1]\n",
    "y_wineQuality = df_wineQuality_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_wineQuality = StandardScaler()\n",
    "X_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(X_wineQuality)\n",
    "y_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(y_wineQuality.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gari\\Desktop\\NEC_Exercise_1.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m y_wineQuality_normalized_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(y_wineQuality_normalized_no_outliers, columns\u001b[39m=\u001b[39m[df_wineQuality\u001b[39m.\u001b[39mcolumns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m X_wineQuality_training_validation \u001b[39m=\u001b[39m X_wineQuality_normalized_df\u001b[39m.\u001b[39miloc[:split_index3, :]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m y_wineQuality_training_validation \u001b[39m=\u001b[39m y_wineQuality_normalized_no_outliers\u001b[39m.\u001b[39;49miloc[:split_index3]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m X_wineQuality_testing \u001b[39m=\u001b[39m X_wineQuality_normalized_df\u001b[39m.\u001b[39miloc[split_index3:, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gari/Desktop/NEC_Exercise_1.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m y_wineQuality_testing \u001b[39m=\u001b[39m y_wineQuality_normalized_no_outliers\u001b[39m.\u001b[39miloc[split_index3:]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Calculate the index to split at\n",
    "split_index1 = int(0.85 * len(df_turbine))\n",
    "# Extract the first 85% for training\n",
    "# Because we normalized with \"MinMaxScaler\", the result is a NumPy array, we conver it to a Pandas DataFrame\n",
    "X_turbine_normalized_df = pd.DataFrame(X_turbine_normalized, columns=df_turbine.columns[:-1])\n",
    "X_turbine_training_validation = X_turbine_normalized_df.iloc[:split_index1, :]\n",
    "y_turbine_training_validation = y_turbine.iloc[:split_index1]\n",
    "# Extract the remaining 15% for testing\n",
    "X_turbine_testing = X_turbine_normalized_df.iloc[split_index1:, :]\n",
    "y_turbine_testing = y_turbine.iloc[split_index1:]\n",
    "\n",
    "#Second dataset, synthetic\n",
    "split_index2 = int(0.80 * len(df_synthetic)) # split 80% for training and validation and 20% for testing\n",
    "X_synthetic_normalized_df = pd.DataFrame(X_synthetic_normalized, columns=df_synthetic.columns[:-1])\n",
    "y_synthetic_normalized_df = pd.DataFrame(y_synthetic_normalized, columns=[df_synthetic.columns[-1]])\n",
    "X_synthetic_training_validation = X_synthetic_normalized_df.iloc[:split_index2, :]\n",
    "y_synthetic_training_validation = y_synthetic_normalized_df.iloc[:split_index2]\n",
    "X_synthetic_testing = X_synthetic_normalized_df.iloc[split_index2:, :]\n",
    "y_synthetic_testing = y_synthetic_normalized_df.iloc[split_index2:]\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "# Calculate the split indices\n",
    "split_index3 = int(0.8 * len(df_wineQuality))\n",
    "# Split the data into training + validation and test sets\n",
    "X_wineQuality_normalized_df = pd.DataFrame(X_wineQuality_normalized_no_outliers, columns=df_wineQuality.columns[:-1])\n",
    "y_wineQuality_normalized_df = pd.DataFrame(y_wineQuality_normalized_no_outliers, columns=[df_wineQuality.columns[-1]])\n",
    "X_wineQuality_training_validation = X_wineQuality_normalized_df.iloc[:split_index3, :]\n",
    "y_wineQuality_training_validation = y_wineQuality_normalized_no_outliers.iloc[:split_index3]\n",
    "X_wineQuality_testing = X_wineQuality_normalized_df.iloc[split_index3:, :]\n",
    "y_wineQuality_testing = y_wineQuality_normalized_no_outliers.iloc[split_index3:]\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "#print(\"Total data size:\", len(df_wineQuality))\n",
    "#print(\"Training data size:\", len(df_wineQualityTrainingValidation))\n",
    "#print(\"Test data size:\", len(df_wineQualityTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementation of BP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Obtaining and comparing predictions using the three models (BP, BP-F, MLR-F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.1: Parameter comparison and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3.2: Model result comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
