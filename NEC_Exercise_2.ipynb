{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "Assignment 2: Classification with SVM, BP and MLR\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the classification in the following three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A2-ring.txt\n",
    "    # Training set 1 : ring-separable.txt\n",
    "    # Training set 2 : ring-merged.txt\n",
    "    # Two different training sets, one easy (separable) and one more difficult (merged)\n",
    "\n",
    "    # Test (valid for set1 and set2): ring-test.txt (Only one test set for both training sets)\n",
    "    # 2 input features + 1 class identifier (0 / 1)\n",
    "    # All data files have 10000 patterns\n",
    "    \n",
    "A2_ring_merged=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-merged.txt', sep='\\t', header=None)\n",
    "A2_ring_separable=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-separable.txt', sep='\\t', header=None)\n",
    "A2_ring_test=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-test.txt', sep='\\t', header=None)\n",
    "\n",
    "df_A2_ring_merged=pd.DataFrame(A2_ring_merged)\n",
    "df_A2_ring_separable=pd.DataFrame(A2_ring_separable)\n",
    "df_A2_ring_test=pd.DataFrame(A2_ring_test)\n",
    "\n",
    "# We plot the two input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dataset: File: A2-bank.txt\n",
    "    # Data: bank-additional.csv (4119 patterns) or bank-additional-full.csv (41188 patterns), we choose one of them (the first is a subset of the second)\n",
    "    # Training: select the first 80% patterns for training\n",
    "    # Test: select the last 20% patterns for test\n",
    "    # Features: 20 features, most of them categorical, you will have to properly represent them as numerical data before training\n",
    "    # Input features: features that refer to the bank client, last contact in the current campaign, other attributes, and social and economic context attributes\n",
    "    # Prediction feature: the last one (yes/no), which corresponds to whether the client has subscribed a term deposit or not\n",
    "    # Observation: missing information is tagged as “unknown”\n",
    "\n",
    "bank_additional=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional.csv', sep=';', header=None)\n",
    "#bank_additional_full=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-full.csv', sep=';', header=None)\n",
    "#bank_additional_names= pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-names.txt', sep=\"\\t\", header=None)\n",
    "\n",
    "df_bank_additional=pd.DataFrame(bank_additional)\n",
    "#df_bank_additional_full=pd.DataFrame(bank_additional_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third dataset: from \"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "    # At least 6 features, one of them used for classification\n",
    "    # he classification feature can be binary or multivariate\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Indian liver patient dataset [584 rows x 11 columns]\n",
    "liver_Disorder=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/Indian Liver Patient Dataset (ILPD).csv', sep=',', header=None)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "df_liver_Disorder=pd.DataFrame(liver_Disorder)\n",
    "\n",
    "header_vector_liver_Disorder = df_liver_Disorder.columns.tolist() #header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"\"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-ring\n",
    "X_A2_ring_separable = df_A2_ring_separable.iloc[:, :-1]  # Features separable (all columns except the last one)\n",
    "y_A2_ring_separable = df_A2_ring_separable.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_merged = df_A2_ring_merged.iloc[:, :-1]  # Features merged\n",
    "y_A2_ring_merged = df_A2_ring_merged.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_test = df_A2_ring_test.iloc[:, :-1]  # Features\n",
    "y_A2_ring_test = df_A2_ring_test.iloc[:, -1]  # Target variable\n",
    "\n",
    "scaler_ring = MinMaxScaler()\n",
    "X_train_ring_separable = scaler_ring.fit_transform(X_A2_ring_separable) # Training set 1\n",
    "y_train_ring_separable = scaler_ring.fit_transform(X_A2_ring_separable)\n",
    "\n",
    "X_train_ring_merged = scaler_ring.fit_transform(X_A2_ring_merged) # Trainig set 2\n",
    "y_train_ring_merged = scaler_ring.fit_transform(y_A2_ring_merged)\n",
    "\n",
    "x_test_ring_normalized = scaler_ring.fit_transform(X_A2_ring_test)\n",
    "y_test_ring_normalized = scaler_ring.fit_transform(y_A2_ring_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalizing the second dataset, needs to be treated.\n",
    "# We have to treat the categorical data and the \"unknown\" values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "X_turbine = df_turbine.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y_turbine = df_turbine.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "scaler_turbine = MinMaxScaler()\n",
    "X_turbine_normalized = scaler_turbine.fit_transform(X_turbine)\n",
    "y_turbine_normalized = scaler_turbine.fit_transform(y_turbine.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liver disorder\n",
    "#By the owners we know that this dataset does not have missing values, we can check by:\n",
    "missing_values_count = df_wineQuality.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wine Quality dataset\n",
    "# No categorical variables in this dataset\n",
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "\n",
    "    # Handle missing values using median imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data_imputed = imputer.fit_transform(data_copy)\n",
    "    \n",
    "    # Convert back to DataFrame with original column names\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "    return data_imputed\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_wineQuality\n",
    "df_wineQuality_no_outliers = handle_outliers_iqr(df_wineQuality)\n",
    "\n",
    "# Shuffle\n",
    "df_wineQuality_shuffled = df_wineQuality_no_outliers.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df_wineQuality_shuffled.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Wine Quality dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wineQuality = df_wineQuality_shuffled.iloc[:, :-1]\n",
    "y_wineQuality = df_wineQuality_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input and output variables\n",
    "scaler_wineQuality = MinMaxScaler()\n",
    "X_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(X_wineQuality)\n",
    "y_wineQuality_normalized_no_outliers = scaler_wineQuality.fit_transform(y_wineQuality.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we divide the datasets into validation & training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First dataset, turbine\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 85% for training\n",
    "# Extract the remaining 15% for testing\n",
    "# Splitting Turbine dataset\n",
    "X_train_turbine, X_test_turbine, y_train_turbine, y_test_turbine = train_test_split(\n",
    "    X_turbine_normalized, y_turbine_normalized, test_size=0.15, shuffle=False\n",
    ")\n",
    "\n",
    "#Second dataset, synthetic\n",
    "X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
    "    X_synthetic_normalized, y_synthetic_normalized, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "#Third dataset, wineQuality\n",
    "X_train_wineQuality, X_test_wineQuality, y_train_wineQuality, y_test_wineQuality = train_test_split(\n",
    "    X_wineQuality_normalized_no_outliers,\n",
    "    y_wineQuality_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Classification problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
