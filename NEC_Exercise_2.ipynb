{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gari\\Desktop\\NEC\\projects\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "\n",
    "Assignment 2: Classification with SVM, BP and MLR\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the classification in the following three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A2-ring.txt\n",
    "    # Training set 1 : ring-separable.txt\n",
    "    # Training set 2 : ring-merged.txt\n",
    "    # Two different training sets, one easy (separable) and one more difficult (merged)\n",
    "\n",
    "    # Test (valid for set1 and set2): ring-test.txt (Only one test set for both training sets)\n",
    "    # 2 input features + 1 class identifier (0 / 1)\n",
    "    # All data files have 10000 patterns\n",
    "    \n",
    "A2_ring_merged=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-merged.txt', sep='\\t', header=None)\n",
    "A2_ring_separable=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-separable.txt', sep='\\t', header=None)\n",
    "A2_ring_test=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-test.txt', sep='\\t', header=None)\n",
    "\n",
    "df_A2_ring_merged=pd.DataFrame(A2_ring_merged)\n",
    "df_A2_ring_separable=pd.DataFrame(A2_ring_separable)\n",
    "df_A2_ring_test=pd.DataFrame(A2_ring_test)\n",
    "\n",
    "# We plot the two input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dataset: File: A2-bank.txt\n",
    "    # Data: bank-additional.csv (4119 patterns) or bank-additional-full.csv (41188 patterns), we choose one of them (the first is a subset of the second)\n",
    "    # Training: select the first 80% patterns for training\n",
    "    # Test: select the last 20% patterns for test\n",
    "    # Features: 20 features, most of them categorical, you will have to properly represent them as numerical data before training\n",
    "    # Input features: features that refer to the bank client, last contact in the current campaign, other attributes, and social and economic context attributes\n",
    "    # Prediction feature: the last one (yes/no), which corresponds to whether the client has subscribed a term deposit or not\n",
    "    # Observation: missing information is tagged as “unknown”\n",
    "\n",
    "bank_additional=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional.csv', sep=';', header=None)\n",
    "#bank_additional_full=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-full.csv', sep=';', header=None)\n",
    "#bank_additional_names= pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-names.txt', sep=\"\\t\", header=None)\n",
    "\n",
    "df_bank_additional=pd.DataFrame(bank_additional)\n",
    "#df_bank_additional_full=pd.DataFrame(bank_additional_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third dataset: from \"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "    # At least 6 features, one of them used for classification\n",
    "    # he classification feature can be binary or multivariate\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Indian liver patient dataset [584 rows x 11 columns]\n",
    "liver_Disorder=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/Indian Liver Patient Dataset (ILPD).csv', sep=',', header=None)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "df_liver_Disorder=pd.DataFrame(liver_Disorder)\n",
    "\n",
    "#We drop the header\n",
    "# Drop the first row\n",
    "df_liver_Disorder = df_liver_Disorder.drop(df_liver_Disorder.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"\"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2-ring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-ring\n",
    "# We normalize the data that has been already splitted\n",
    "X_A2_ring_separable = df_A2_ring_separable.iloc[:, :-1]  # Features separable (all columns except the last one)\n",
    "y_A2_ring_separable = df_A2_ring_separable.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_merged = df_A2_ring_merged.iloc[:, :-1]  # Features merged\n",
    "y_A2_ring_merged = df_A2_ring_merged.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_test = df_A2_ring_test.iloc[:, :-1]  # Features\n",
    "y_A2_ring_test = df_A2_ring_test.iloc[:, -1]  # Target variable\n",
    "\n",
    "scaler_ring = MinMaxScaler()\n",
    "X_train_ring_separable = scaler_ring.fit_transform(X_A2_ring_separable) # Training set 1\n",
    "# Reshape the array to a 2D shape (required by MinMaxScaler)\n",
    "y_train_ring_separable = scaler_ring.fit_transform(y_A2_ring_separable.values.reshape(-1, 1))\n",
    "\n",
    "X_train_ring_merged = scaler_ring.fit_transform(X_A2_ring_merged) # Trainig set 2\n",
    "y_train_ring_merged = scaler_ring.fit_transform(y_A2_ring_merged.values.reshape(-1, 1))\n",
    "\n",
    "X_test_ring_normalized = scaler_ring.fit_transform(X_A2_ring_test) # Test\n",
    "y_test_ring_normalized = scaler_ring.fit_transform(y_A2_ring_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2-bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "\n",
    "# Before normalizing the second dataset, it needs to be treated.\n",
    "# We have to treat the categorical data and the \"unknown\" values.\n",
    "\n",
    "# Treatment of the categorical data and the \"unknown\" values\n",
    "# Replace \"unknown\" values with NaN\n",
    "df_bank_additional.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# Handling missing values\n",
    "df_bank_additional.fillna(df_bank_additional.mode().iloc[0], inplace=True)\n",
    "\n",
    "# Extract the column names from the first row\n",
    "df_bank_additional.columns = df_bank_additional.iloc[0]\n",
    "df_bank_additional = df_bank_additional[1:]\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df_bank_additional.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_bank_additional[column] = label_encoder.fit_transform(df_bank_additional[column])\n",
    "\n",
    "# Separate the target variable\n",
    "X_bank_additional = df_bank_additional.drop(\"y\", axis=1)  # Features\n",
    "y_bank_additional = df_bank_additional[\"y\"]  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "# Now, we normalize the data\n",
    "scaler_bank = MinMaxScaler()\n",
    "X_train_bank_additional = scaler_bank.fit_transform(X_bank_additional) # bank additional\n",
    "# Reshape the array to a 2D shape (required by MinMaxScaler)\n",
    "y_train_bank_additional = scaler_bank.fit_transform(y_bank_additional.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 80% for training\n",
    "# Extract the remaining 20% for testing\n",
    "# Splitting A2-bank dataset\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = train_test_split(\n",
    "    X_train_bank_additional, y_train_bank_additional, test_size=0.20, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liver disorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 4\n"
     ]
    }
   ],
   "source": [
    "# Liver disorder\n",
    "# We check if this dataset have missing values:\n",
    "missing_values_count = df_liver_Disorder.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the categorical data into numerical\n",
    "# Observing the dataset, there is a column with categorical data, which is the \"Gender\" with the name 1\n",
    "gender_mapping = {'Female': 0, 'Male': 1}\n",
    "df_liver_Disorder[1] = df_liver_Disorder[1].replace(gender_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We handle missing values using median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_liver_Disorder_imputed = pd.DataFrame(imputer.fit_transform(df_liver_Disorder), columns=df_liver_Disorder.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = df_liver_Disorder_imputed.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "\n",
    "    # Handle missing values using median imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data_imputed = imputer.fit_transform(data_copy)\n",
    "    \n",
    "    # Convert back to DataFrame with original column names\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "    return data_imputed\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_liver_Disorder\n",
    "df_liver_Disorder_no_outliers = handle_outliers_iqr(df_liver_Disorder_imputed)\n",
    "\n",
    "# Shuffle\n",
    "df_liver_Disorder_shuffled = df_liver_Disorder_no_outliers.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = df_liver_Disorder_shuffled.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_liver_Disorder = df_liver_Disorder_shuffled.iloc[:, :-1]\n",
    "y_liver_Disorder = df_liver_Disorder_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input variables\n",
    "scaler_liver_Disorder = MinMaxScaler()\n",
    "X_liver_Disorder_normalized_no_outliers = scaler_liver_Disorder.fit_transform(X_liver_Disorder)\n",
    "y_liver_Disorder_normalized_no_outliers = scaler_liver_Disorder.fit_transform(y_liver_Disorder.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third dataset, Liver disorder\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 80% for training\n",
    "# Extract the remaining 20% for testing\n",
    "# Splitting liver_Disorder dataset\n",
    "X_train_liver_Disorder, X_test_liver_Disorder, y_train_liver_Disorder, y_test_liver_Disorder = train_test_split(\n",
    "    X_liver_Disorder_normalized_no_outliers,\n",
    "    y_liver_Disorder_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform supervised training of 3 classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (support vector machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of SVM: kernel, and parameters\n",
    "def evaluate_svm(X_train, y_train, X_test, y_test, kernel, C):\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    # Create an SVM classifier with the desired parameters\n",
    "    svm_classifier = SVC(kernel=kernel, C=C)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(svm_classifier, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    expected_error = 1 - np.mean(cv_scores)  # Expected classification error\n",
    "    \n",
    "    # Train the SVM classifier on the entire training set\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate the classification error on the test set\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return expected_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP (back propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class CustomKerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.0, activation='relu', epochs=100, batch_size=32, verbose=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Define model architecture\n",
    "        self.model = Sequential([\n",
    "            Dense(32, activation=self.activation),\n",
    "            Dense(10, activation=self.activation),\n",
    "            Dense(5, activation=self.activation),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        self.model.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "        # Fit model\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of BP: architecture of the network, learning rate and momentum, activation function, and number of epochs\n",
    "\n",
    "def evaluate_bp(X_train, y_train, X_test, y_test, learning_rate, momentum, activation, epochs):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    def create_model():\n",
    "        model = Sequential([\n",
    "            Dense(32, activation=activation),\n",
    "            Dense(10, activation=activation),\n",
    "            Dense(5, activation=activation),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    model = CustomKerasClassifier(epochs=epochs, \n",
    "                                  batch_size=32, \n",
    "                                  verbose=0,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  momentum=momentum,\n",
    "                                  activation=activation)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    expected_error = 1 - np.mean(cv_scores)  # Expected classification error\n",
    "    \n",
    "    # Fit the model to the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the classification error on the test set\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred.round())\n",
    "    \n",
    "    return expected_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR (multi-linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlr(X_train, y_train, X_test, y_test, C, solver):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a logistic regression classifier with the current parameters\n",
    "    logistic_regression = LogisticRegression(C=C, solver=solver)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(logistic_regression, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    expected_error = 1 - np.mean(cv_scores)  # Expected classification error\n",
    "    \n",
    "    # Train the logistic regression classifier on the entire training set\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    y_pred = logistic_regression.predict(X_test)\n",
    "    \n",
    "    # Calculate the classification error on the test set\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return expected_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic process to find the best parameters (kernel, C) for the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_svm(X_train, y_train, X_test, y_test):\n",
    "    # Set initial parameters\n",
    "    C_values = [0.1, 1, 10, 100, 1000]  # Values for C\n",
    "    kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']  # Values for kernel\n",
    "    best_test_error = float('inf')\n",
    "    best_expected_error = float('inf')\n",
    "    best_params = {'kernel': None, 'C': None}\n",
    "\n",
    "    # Tune C\n",
    "    for C in C_values:\n",
    "        # Calculate the classification error\n",
    "        expected_error, test_error = evaluate_svm(X_train, y_train, X_test, y_test, kernel=kernel_values[0], C=C)\n",
    "        \n",
    "        # Update best error and parameters if current error is lower\n",
    "        if test_error < best_test_error:\n",
    "            best_test_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_params['C'] = C\n",
    "        \n",
    "    best_test_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    # Tune kernel\n",
    "    for kernel in kernel_values:\n",
    "        expected_error, test_error = evaluate_svm(X_train, y_train, X_test, y_test, kernel=kernel, C=best_params['C'])\n",
    "        \n",
    "        # Update best error and parameters if current error is lower\n",
    "        if test_error < best_test_error:\n",
    "            best_test_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_params['kernel'] = kernel\n",
    "    \n",
    "    best_expected_error, best_test_error = evaluate_svm(X_train, y_train, X_test, y_test, kernel=best_params['kernel'], C=best_params['C'])\n",
    "\n",
    "    return best_params, best_expected_error, best_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic process to find the best parameters (learning_rate, momentum, activation, epochs) for the BP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_backpropagation(X_train, y_train, X_test, y_test):\n",
    "    # Set initial parameters\n",
    "    learning_rates = [0.2, 0.15, 0.1, 0.015, 0.01]  # Values for learning rate\n",
    "    momentums = [0.0, 0.3, 0.6, 0.9]  # Values for momentum\n",
    "    activations = ['sigmoid', 'relu', 'linear', 'tanh']  # Values for activation\n",
    "    epochs_values = [50, 100, 200, 500, 1000] # Number of epochs\n",
    "\n",
    "    best_error  = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    best_hyperparameters = {'learning_rate': None, 'momentum': None, 'activation': None, 'epochs': None}\n",
    "\n",
    "    # Tune learning rate\n",
    "    for learning_rate in learning_rates:\n",
    "        expected_error, test_error = evaluate_bp(X_train, y_train, X_test, y_test, learning_rate, momentums[0], activations[0], epochs_values[0])\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_hyperparameters['learning_rate'] = learning_rate\n",
    "\n",
    "    best_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    # Tune momentum\n",
    "    for momentum in momentums:\n",
    "        expected_error, test_error = evaluate_bp(X_train, y_train, X_test, y_test, best_hyperparameters['learning_rate'], momentum, activations[0], epochs_values[0])\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_hyperparameters['momentum'] = momentum\n",
    "\n",
    "    best_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    # Tune activation\n",
    "    for activation in activations:\n",
    "        expected_error, test_error = evaluate_bp(X_train, y_train, X_test, y_test, best_hyperparameters['learning_rate'], best_hyperparameters['momentum'], activation, epochs_values[0])\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_hyperparameters['activation'] = activation\n",
    "\n",
    "    best_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    # Tune epochs\n",
    "    for epochs in epochs_values:\n",
    "        expected_error, test_error = evaluate_bp(X_train, y_train, X_test, y_test, best_hyperparameters['learning_rate'], best_hyperparameters['momentum'], best_hyperparameters['activation'], epochs)\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_hyperparameters['epochs'] = epochs\n",
    "\n",
    "    best_expected_error, best_error = evaluate_svm(X_train, y_train, X_test, y_test, best_hyperparameters['learning_rate'], best_hyperparameters['momentum'], best_hyperparameters['activation'], best_hyperparameters['epochs'])\n",
    "\n",
    "    return best_hyperparameters, best_expected_error, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic process to find the best parameters (C, solver) for the MLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    import warnings\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import numpy as np\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Set initial parameters\n",
    "    C_values = [0.01, 0.1, 1, 10, 100]  # Values for regularization parameter C\n",
    "    solver_values = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Values for the optimization algorithm\n",
    "\n",
    "    best_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    best_params = {'C': None, 'solver': None}\n",
    "\n",
    "    # Tune C\n",
    "    for C in C_values:\n",
    "        # Calculate the classification error\n",
    "        expected_error, test_error = evaluate_mlr(X_train, y_train, X_test, y_test, C=C, solver=solver_values[0])\n",
    "\n",
    "        # Update best error and C if current error is lower\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_params['C'] = C\n",
    "\n",
    "    best_error = float('inf')  # Initialize with a large value\n",
    "    best_expected_error = float('inf')\n",
    "    # Tune solver\n",
    "    for solver in solver_values:\n",
    "        expected_error, test_error = evaluate_mlr(X_train, y_train, X_test, y_test, C=best_params['C'], solver=solver)\n",
    "        # Update best error and solver if current error is lower\n",
    "        if test_error < best_error:\n",
    "            best_error = test_error\n",
    "            best_expected_error = expected_error\n",
    "            best_params['solver'] = solver\n",
    "\n",
    "    # Calculate error for best parameters\n",
    "    best_expected_error, best_error = evaluate_mlr(X_train, y_train, X_test, y_test, C=best_params['C'], solver=best_params['solver'])\n",
    "\n",
    "    return best_params, best_expected_error, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the before functions to finde the best parameters for our datasets for the three models (SVM, BP, MLR):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1: A2-ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_ring_merged\n",
    "y_train = y_train_ring_merged\n",
    "X_test = X_test_ring_normalized\n",
    "y_test = y_test_ring_normalized\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#SVM\n",
    "best_params_svm_D1, best_expected_error_svm, best_error_svm = tune_svm(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for SVM:\", best_params_svm_D1)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_svm)\n",
    "print(\"Best classification error on the test set for SVM:\", best_error_svm)\n",
    "\n",
    "# BP\n",
    "best_params_bp_D1, best_expected_error_bp, best_error_bp = tune_backpropagation(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for BP:\", best_params_bp_D1)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_bp)\n",
    "print(\"Best classification error on the test set for BP:\", best_error_bp)\n",
    "\n",
    "#MLR\n",
    "best_params_mlr_D1, best_expected_error_mlr, best_error_mlr = tune_logistic_regression(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for MLR:\", best_params_mlr_D1)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_mlr)\n",
    "print(\"Best classification error on the test set for MLR:\", best_error_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2: A2-bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_bank\n",
    "y_train = y_train_bank\n",
    "X_test = X_test_bank\n",
    "y_test = y_test_bank\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#SVM\n",
    "best_params_svm_D2, best_expected_error_svm, best_error_svm = tune_svm(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for SVM:\", best_params_svm_D2)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_svm)\n",
    "print(\"Best classification error on the test set for SVM:\", best_error_svm)\n",
    "\n",
    "# BP\n",
    "best_params_bp_D2, best_expected_error_bp, best_error_bp = tune_backpropagation(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for BP:\", best_params_bp_D2)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_bp)\n",
    "print(\"Best classification error on the test set for BP:\", best_error_bp)\n",
    "\n",
    "#MLR\n",
    "best_params_mlr_D2, best_expected_error_mlr, best_error_mlr = tune_logistic_regression(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for MLR:\", best_params_mlr_D2)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_mlr)\n",
    "print(\"Best classification error on the test set for MLR:\", best_error_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 3: liver disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_liver_Disorder\n",
    "y_train = y_train_liver_Disorder\n",
    "X_test = X_test_liver_Disorder\n",
    "y_test = y_test_liver_Disorder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#SVM\n",
    "best_params_svm_D3, best_expected_error_svm, best_error_svm = tune_svm(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for SVM:\", best_params_svm_D3)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_svm)\n",
    "print(\"Best classification error on the test set for SVM:\", best_error_svm)\n",
    "\n",
    "# BP\n",
    "best_params_bp_D3, best_expected_error_bp, best_error_bp = tune_backpropagation(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for BP:\", best_params_bp_D3)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_bp)\n",
    "print(\"Best classification error on the test set for BP:\", best_error_bp)\n",
    "\n",
    "#MLR\n",
    "best_params_mlr_D3, best_expected_error_mlr, best_error_mlr = tune_logistic_regression(X_train, y_train, X_test, y_test)\n",
    "print(\"Best parameters for MLR:\", best_params_mlr_D3)\n",
    "print(\"Expected classification error obtained from cross-validation:\", best_expected_error_mlr)\n",
    "print(\"Best classification error on the test set for MLR:\", best_error_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2.2: Evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification error % on the Test and Validation sets for SVM, BP and MLR, and for all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification error % \n",
    "# Import necessary libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a function to compute the classification error\n",
    "def compute_classification_error(y_true, y_pred):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Extract the elements of the confusion matrix\n",
    "    n_01 = cm[0][1]\n",
    "    n_10 = cm[1][0]\n",
    "    n_00 = cm[0][0]\n",
    "    n_11 = cm[1][1]\n",
    "    # Compute the classification error\n",
    "    error = 100 * ((n_01 + n_10) / (n_00 + n_11 + n_01 + n_10))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_bp(X_train, y_train, learning_rate, momentum, activation, epochs):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    \n",
    "    # Create a simple feedforward neural network for binary classification\n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=X_train.shape[1], activation=activation),\n",
    "        Dense(10, activation=activation),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification, so using sigmoid activation\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "                  loss='binary_crossentropy',  # Binary classification, so using binary crossentropy loss\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate classifiers for each dataset\n",
    "def evaluate_classifiers(X_train, y_train, X_test, y_test, X_val, y_val, best_params_svm, best_params_bp, best_params_mlr):\n",
    "    # SVM classifier\n",
    "    kernel = best_params_svm['kernel']\n",
    "    C = best_params_svm['C']\n",
    "    # Train SVM classifier with the best parameters\n",
    "    svm_classifier = SVC(kernel=kernel, C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    # Predict labels for Test set using SVM\n",
    "    svm_test_pred = svm_classifier.predict(X_test)\n",
    "    # Compute classification error for Test set using SVM\n",
    "    svm_test_error = compute_classification_error(y_test, svm_test_pred)\n",
    "    # Predict labels for Validation set using SVM\n",
    "    svm_val_pred = svm_classifier.predict(X_val)\n",
    "    # Compute classification error for Validation set using SVM\n",
    "    svm_val_error = compute_classification_error(y_val, svm_val_pred)\n",
    "\n",
    "\n",
    "    # BP classifier\n",
    "    learning_rate_bp = best_params_bp['learning_rate']\n",
    "    momentum_bp = best_params_bp['momentum']\n",
    "    activation_bp = best_params_bp['activation']\n",
    "    epochs_bp = best_params_bp['epochs']\n",
    "    # Train BP classifier\n",
    "    bp_classifier = create_and_train_bp(X_train, y_train, learning_rate_bp, momentum_bp, activation_bp, epochs_bp)\n",
    "    # Predict labels for Test set using BP\n",
    "    bp_test_pred = bp_classifier.predict(X_test)\n",
    "    # Compute classification error for Test set using BP\n",
    "    bp_test_error = compute_classification_error(y_test, bp_test_pred)\n",
    "    # Predict labels for Validation set using BP\n",
    "    bp_val_pred = bp_classifier.predict(X_val)\n",
    "    # Compute classification error for Validation set using BP\n",
    "    bp_val_error = compute_classification_error(y_val, bp_val_pred)\n",
    "\n",
    "\n",
    "    # MLR classifier\n",
    "    C_mlr = best_params_mlr['C']\n",
    "    solver_mlr = best_params_mlr['solver']\n",
    "    # Train MLR classifier\n",
    "    mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "    mlr_classifier.fit(X_train, y_train)\n",
    "    # Predict labels for Test set using MLR\n",
    "    mlr_test_pred = mlr_classifier.predict(X_test)\n",
    "    # Compute classification error for Test set using MLR\n",
    "    mlr_test_error = compute_classification_error(y_test, mlr_test_pred)\n",
    "    # Predict labels for Validation set using MLR\n",
    "    mlr_val_pred = mlr_classifier.predict(X_val)\n",
    "    # Compute classification error for Validation set using MLR\n",
    "    mlr_val_error = compute_classification_error(y_val, mlr_val_pred)\n",
    "\n",
    "    return (svm_test_error, svm_val_error), (bp_test_error, bp_val_error), (mlr_test_error, mlr_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We split the data into training and validation\n",
    "# Split the training data into training and validation sets\n",
    "X_train_dataset1, y_train_dataset1, X_val_dataset1, y_val_dataset1 = train_test_split(X_train_ring_merged, y_train_ring_merged, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second dataset\n",
    "X_train_dataset2, y_train_dataset2, X_val_dataset2, y_val_dataset2 = train_test_split(X_train_bank, y_train_bank, test_size=0.2, random_state=42)\n",
    "\n",
    "# Third dataset\n",
    "X_train_dataset3, y_train_dataset3, X_val_dataset3, y_val_dataset3 = train_test_split(X_train_liver_Disorder, y_train_liver_Disorder, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate_classifiers function for each dataset\n",
    "# Replace X_train, y_train, X_test, y_test, X_val, y_val with actual data for each dataset\n",
    "dataset1_errors = evaluate_classifiers(X_train_dataset1, y_train_dataset1, X_test_ring_normalized, y_test_ring_normalized, X_val_dataset1, y_val_dataset1, best_params_svm_D1, best_params_bp_D1, best_params_mlr_D1)\n",
    "dataset2_errors = evaluate_classifiers(X_train_dataset2, y_train_dataset2, X_test_bank, y_test_bank, X_val_dataset2, y_val_dataset2, best_params_svm_D2, best_params_bp_D2, best_params_mlr_D2)\n",
    "dataset3_errors = evaluate_classifiers(X_train_dataset3, y_train_dataset3, X_test_liver_Disorder, y_test_liver_Disorder, X_val_dataset3, y_val_dataset3, best_params_svm_D3, best_params_bp_D3, best_params_mlr_D3)\n",
    "\n",
    "# Print or store the classification errors for each dataset and classifier\n",
    "print(\"Dataset 1 Errors (SVM Test, SVM Validation), (BP Test, BP Validation), (MLR Test, MLR Validation):\", dataset1_errors)\n",
    "print(\"Dataset 2 Errors (SVM Test, SVM Validation), (BP Test, BP Validation), (MLR Test, MLR Validation):\", dataset2_errors)\n",
    "print(\"Dataset 3 Errors (SVM Test, SVM Validation), (BP Test, BP Validation), (MLR Test, MLR Validation):\", dataset3_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and compare the confusion matrices of the results obtained by the three algorithms for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def compute_confusion_matrices(X_train, y_train, X_test, y_test, best_params_svm, best_params_bp, best_params_mlr):\n",
    "    # Train SVM classifier\n",
    "    kernel = best_params_svm['kernel']\n",
    "    C = best_params_svm['C']\n",
    "    svm_classifier = SVC(kernel=kernel, C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    svm_test_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # Train BP classifier\n",
    "    learning_rate_bp = best_params_bp['learning_rate']\n",
    "    momentum_bp = best_params_bp['momentum']\n",
    "    activation_bp = best_params_bp['activation']\n",
    "    epochs_bp = best_params_bp['epochs']\n",
    "    bp_classifier = MLPClassifier(hidden_layer_sizes=(32, 10, 5), activation=activation_bp, solver='sgd',\n",
    "                                   learning_rate_init=learning_rate_bp, momentum=momentum_bp, max_iter=epochs_bp)\n",
    "    bp_classifier.fit(X_train, y_train)\n",
    "    bp_test_pred = bp_classifier.predict(X_test)\n",
    "\n",
    "    # Train MLR classifier\n",
    "    C_mlr = best_params_mlr['C']\n",
    "    solver_mlr = best_params_mlr['solver']\n",
    "    mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "    mlr_classifier.fit(X_train, y_train)\n",
    "    mlr_test_pred = mlr_classifier.predict(X_test)\n",
    "\n",
    "    # Compute confusion matrices\n",
    "    svm_conf_matrix = confusion_matrix(y_test, svm_test_pred)\n",
    "    bp_conf_matrix = confusion_matrix(y_test, bp_test_pred)\n",
    "    mlr_conf_matrix = confusion_matrix(y_test, mlr_test_pred)\n",
    "\n",
    "    return svm_conf_matrix, bp_conf_matrix, mlr_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "svm_conf_matrix, bp_conf_matrix, mlr_conf_matrix = compute_confusion_matrices(X_train_ring_merged, y_train_ring_merged, X_test_ring_normalized, y_test_ring_normalized, best_params_svm_D1, best_params_bp_D1, best_params_mlr_D1)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(svm_conf_matrix)\n",
    "\n",
    "print(\"\\nBP Confusion Matrix:\")\n",
    "print(bp_conf_matrix)\n",
    "\n",
    "print(\"\\nMLR Confusion Matrix:\")\n",
    "print(mlr_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2\n",
    "svm_conf_matrix, bp_conf_matrix, mlr_conf_matrix = compute_confusion_matrices(X_train_bank, y_train_bank, X_test_bank, y_test_bank, best_params_svm_D2, best_params_bp_D2, best_params_mlr_D2)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(svm_conf_matrix)\n",
    "\n",
    "print(\"\\nBP Confusion Matrix:\")\n",
    "print(bp_conf_matrix)\n",
    "\n",
    "print(\"\\nMLR Confusion Matrix:\")\n",
    "print(mlr_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3\n",
    "svm_conf_matrix, bp_conf_matrix, mlr_conf_matrix = compute_confusion_matrices(X_train_liver_Disorder, y_train_liver_Disorder, X_test_liver_Disorder, y_test_liver_Disorder, best_params_svm_D3, best_params_bp_D3, best_params_mlr_D3)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(svm_conf_matrix)\n",
    "\n",
    "print(\"\\nBP Confusion Matrix:\")\n",
    "print(bp_conf_matrix)\n",
    "\n",
    "print(\"\\nMLR Confusion Matrix:\")\n",
    "print(mlr_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the ROC curve and the AUC for the Test set by just varying a threshold required to transform predictions into classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curves_bp_mlr(X_train, y_train, X_test, y_test, best_params_bp, best_params_mlr):\n",
    "    # Train BP classifier\n",
    "    learning_rate_bp = best_params_bp['learning_rate']\n",
    "    momentum_bp = best_params_bp['momentum']\n",
    "    activation_bp = best_params_bp['activation']\n",
    "    epochs_bp = best_params_bp['epochs']\n",
    "    bp_classifier = create_and_train_bp(X_train, y_train, learning_rate_bp, momentum_bp, activation_bp, epochs_bp)\n",
    "    bp_test_pred = bp_classifier.predict(X_test)\n",
    "\n",
    "    # Train MLR classifier\n",
    "    C_mlr = best_params_mlr['C']\n",
    "    solver_mlr = best_params_mlr['solver']\n",
    "    mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "    mlr_classifier.fit(X_train, y_train)\n",
    "    mlr_test_pred_probs = mlr_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n",
    "\n",
    "    # Calculate ROC curve and AUC for BP classifier\n",
    "    fpr_bp, tpr_bp, thresholds_bp = roc_curve(y_test, bp_test_pred)\n",
    "    auc_bp = auc(fpr_bp, tpr_bp)\n",
    "\n",
    "    # Calculate ROC curve and AUC for MLR classifier\n",
    "    fpr_mlr, tpr_mlr, thresholds_mlr = roc_curve(y_test, mlr_test_pred_probs)\n",
    "    auc_mlr = auc(fpr_mlr, tpr_mlr)\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_bp, tpr_bp, color='blue', lw=2, label=f'BP (AUC = {auc_bp:.2f})')\n",
    "    plt.plot(fpr_mlr, tpr_mlr, color='green', lw=2, label=f'MLR (AUC = {auc_mlr:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for BP and MLR')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "plot_roc_curves_bp_mlr(X_train_ring_merged, y_train_ring_merged, X_test_ring_normalized, y_test_ring_normalized, best_params_bp_D1, best_params_mlr_D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2\n",
    "plot_roc_curves_bp_mlr(X_train_bank, y_train_bank, X_test_bank, y_test_bank, best_params_bp_D2, best_params_mlr_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3\n",
    "plot_roc_curves_bp_mlr(X_train_liver_Disorder, y_train_liver_Disorder, X_test_liver_Disorder, y_test_liver_Disorder, best_params_bp_D3, best_params_mlr_D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the data to assess the performance in the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_classification_results(X_train, y_train, X_test, y_test, classifier):\n",
    "    # Perform dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Plot training data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label in np.unique(y_train):\n",
    "        plt.scatter(X_train_pca[y_train == label, 0], X_train_pca[y_train == label, 1], label=f'Class {label}')\n",
    "    plt.title('Training Data')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot test data with predictions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label in np.unique(y_test):\n",
    "        plt.scatter(X_test_pca[y_test == label, 0], X_test_pca[y_test == label, 1], label=f'Class {label}', alpha=0.5)\n",
    "    plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=classifier.predict(X_test), cmap='viridis', marker='x', label='Predictions')\n",
    "    plt.title('Test Data with Predictions')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = best_params_svm_D1['kernel']\n",
    "C = best_params_svm_D1['C']\n",
    "svm_classifier = SVC(kernel=kernel, C=C)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "learning_rate_bp = best_params_bp_D1['learning_rate']\n",
    "momentum_bp = best_params_bp_D1['momentum']\n",
    "activation_bp = best_params_bp_D1['activation']\n",
    "epochs_bp = best_params_bp_D1['epochs']\n",
    "bp_classifier = MLPClassifier(hidden_layer_sizes=(32, 10, 5), activation=activation_bp, solver='sgd',\n",
    "                                   learning_rate_init=learning_rate_bp, momentum=momentum_bp, max_iter=epochs_bp)\n",
    "bp_classifier.fit(X_train, y_train)\n",
    "\n",
    "C_mlr = best_params_mlr_D1['C']\n",
    "solver_mlr = best_params_mlr_D1['solver']\n",
    "mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "mlr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Dataset 1\n",
    "visualize_classification_results(X_train_ring_merged, y_train_ring_merged, X_test_ring_normalized, y_test_ring_normalized, svm_classifier)\n",
    "visualize_classification_results(X_train_ring_merged, y_train_ring_merged, X_test_ring_normalized, y_test_ring_normalized, bp_classifier)\n",
    "visualize_classification_results(X_train_ring_merged, y_train_ring_merged, X_test_ring_normalized, y_test_ring_normalized, mlr_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = best_params_svm_D2['kernel']\n",
    "C = best_params_svm_D2['C']\n",
    "svm_classifier = SVC(kernel=kernel, C=C)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "learning_rate_bp = best_params_bp_D2['learning_rate']\n",
    "momentum_bp = best_params_bp_D2['momentum']\n",
    "activation_bp = best_params_bp_D2['activation']\n",
    "epochs_bp = best_params_bp_D2['epochs']\n",
    "bp_classifier = MLPClassifier(hidden_layer_sizes=(32, 10, 5), activation=activation_bp, solver='sgd',\n",
    "                                   learning_rate_init=learning_rate_bp, momentum=momentum_bp, max_iter=epochs_bp)\n",
    "bp_classifier.fit(X_train, y_train)\n",
    "\n",
    "C_mlr = best_params_mlr_D2['C']\n",
    "solver_mlr = best_params_mlr_D2['solver']\n",
    "mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "mlr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Dataset 2\n",
    "visualize_classification_results(X_train_bank, y_train_bank, X_test_bank, y_test_bank, svm_classifier)\n",
    "visualize_classification_results(X_train_bank, y_train_bank, X_test_bank, y_test_bank, bp_classifier)\n",
    "visualize_classification_results(X_train_bank, y_train_bank, X_test_bank, y_test_bank, mlr_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = best_params_svm_D3['kernel']\n",
    "C = best_params_svm_D3['C']\n",
    "svm_classifier = SVC(kernel=kernel, C=C)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "learning_rate_bp = best_params_bp_D3['learning_rate']\n",
    "momentum_bp = best_params_bp_D3['momentum']\n",
    "activation_bp = best_params_bp_D3['activation']\n",
    "epochs_bp = best_params_bp_D3['epochs']\n",
    "bp_classifier = MLPClassifier(hidden_layer_sizes=(32, 10, 5), activation=activation_bp, solver='sgd',\n",
    "                                   learning_rate_init=learning_rate_bp, momentum=momentum_bp, max_iter=epochs_bp)\n",
    "bp_classifier.fit(X_train, y_train)\n",
    "\n",
    "C_mlr = best_params_mlr['C']\n",
    "solver_mlr = best_params_mlr['solver']\n",
    "mlr_classifier = LogisticRegression(C=C_mlr, solver=solver_mlr)\n",
    "mlr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Dataset 3\n",
    "visualize_classification_results(X_train_liver_Disorder, y_train_liver_Disorder, X_test_liver_Disorder, y_test_liver_Disorder, svm_classifier)\n",
    "visualize_classification_results(X_train_liver_Disorder, y_train_liver_Disorder, X_test_liver_Disorder, y_test_liver_Disorder, bp_classifier)\n",
    "visualize_classification_results(X_train_liver_Disorder, y_train_liver_Disorder, X_test_liver_Disorder, y_test_liver_Disorder, mlr_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
