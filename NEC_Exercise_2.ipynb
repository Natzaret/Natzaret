{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gari\\Desktop\\NEC\\projects\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URV                                                                            MESIIA\n",
    "\n",
    "Neural and Evolutionary Computation (NEC)\n",
    "\n",
    "Assignment 2: Classification with SVM, BP and MLR\n",
    "\n",
    "Teachers: Dr. Jordi Duch, Dr. Sergio Gomez\n",
    "\n",
    "Student: Natzaret Gálvez Rísquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Selecting and analyzing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the classification in the following three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upload the datasets\n",
    "\n",
    "# First dataset: File: A2-ring.txt\n",
    "    # Training set 1 : ring-separable.txt\n",
    "    # Training set 2 : ring-merged.txt\n",
    "    # Two different training sets, one easy (separable) and one more difficult (merged)\n",
    "\n",
    "    # Test (valid for set1 and set2): ring-test.txt (Only one test set for both training sets)\n",
    "    # 2 input features + 1 class identifier (0 / 1)\n",
    "    # All data files have 10000 patterns\n",
    "    \n",
    "A2_ring_merged=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-merged.txt', sep='\\t', header=None)\n",
    "A2_ring_separable=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-separable.txt', sep='\\t', header=None)\n",
    "A2_ring_test=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-ring/A2-ring-test.txt', sep='\\t', header=None)\n",
    "\n",
    "df_A2_ring_merged=pd.DataFrame(A2_ring_merged)\n",
    "df_A2_ring_separable=pd.DataFrame(A2_ring_separable)\n",
    "df_A2_ring_test=pd.DataFrame(A2_ring_test)\n",
    "\n",
    "# We plot the two input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dataset: File: A2-bank.txt\n",
    "    # Data: bank-additional.csv (4119 patterns) or bank-additional-full.csv (41188 patterns), we choose one of them (the first is a subset of the second)\n",
    "    # Training: select the first 80% patterns for training\n",
    "    # Test: select the last 20% patterns for test\n",
    "    # Features: 20 features, most of them categorical, you will have to properly represent them as numerical data before training\n",
    "    # Input features: features that refer to the bank client, last contact in the current campaign, other attributes, and social and economic context attributes\n",
    "    # Prediction feature: the last one (yes/no), which corresponds to whether the client has subscribed a term deposit or not\n",
    "    # Observation: missing information is tagged as “unknown”\n",
    "\n",
    "bank_additional=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional.csv', sep=';', header=None)\n",
    "#bank_additional_full=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-full.csv', sep=';', header=None)\n",
    "#bank_additional_names= pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/A2-bank/bank-additional-names.txt', sep=\"\\t\", header=None)\n",
    "\n",
    "df_bank_additional=pd.DataFrame(bank_additional)\n",
    "#df_bank_additional_full=pd.DataFrame(bank_additional_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third dataset: from \"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "    # At least 6 features, one of them used for classification\n",
    "    # he classification feature can be binary or multivariate\n",
    "    # At least 400 patterns\n",
    "    # Select randomly 80% of the patterns for training and validation, and the remaining 20% for test; it is important to shuffle the original data, to destroy any kind of sorting it could have\n",
    "\n",
    "# Indian liver patient dataset [584 rows x 11 columns]\n",
    "liver_Disorder=pd.read_csv('C:/Users/Gari/Desktop/Assignments_NEC/A2/Indian Liver Patient Dataset (ILPD).csv', sep=',', header=None)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "df_liver_Disorder=pd.DataFrame(liver_Disorder)\n",
    "\n",
    "#We drop the header\n",
    "# Drop the first row\n",
    "df_liver_Disorder = df_liver_Disorder.drop(df_liver_Disorder.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the data preprocessing to later do the data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values, we check for and handle any missing values in our datasets\n",
    "# Categorical values, if there are categorical variables, we encode them appropriately\n",
    "# Outliers, we identify and handle the outliers in the data\n",
    "# Normalization, in case is needed\n",
    "\n",
    "# Data Preprocessing for Dataset 1 and 2\n",
    "# - Normalize input and output variables\n",
    "# - No need to preprocess (datasets already cleaned)\n",
    "\n",
    "# Data Preprocessing for Dataset 3\n",
    "# - Link to the source webpage to the documentation: \"\"https://www.kaggle.com/datasets/fatemehmehrparvar/liver-disorders?resource=download\"\n",
    "# - Check for missing values, represent categorical values, look for outliers\n",
    "# - Normalize input/output variables if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2-ring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-ring\n",
    "# We normalize the data that has been already splitted\n",
    "X_A2_ring_separable = df_A2_ring_separable.iloc[:, :-1]  # Features separable (all columns except the last one)\n",
    "y_A2_ring_separable = df_A2_ring_separable.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_merged = df_A2_ring_merged.iloc[:, :-1]  # Features merged\n",
    "y_A2_ring_merged = df_A2_ring_merged.iloc[:, -1]  # Target variable\n",
    "\n",
    "X_A2_ring_test = df_A2_ring_test.iloc[:, :-1]  # Features\n",
    "y_A2_ring_test = df_A2_ring_test.iloc[:, -1]  # Target variable\n",
    "\n",
    "scaler_ring = MinMaxScaler()\n",
    "X_train_ring_separable = scaler_ring.fit_transform(X_A2_ring_separable) # Training set 1\n",
    "# Reshape the array to a 2D shape (required by MinMaxScaler)\n",
    "y_train_ring_separable = scaler_ring.fit_transform(y_A2_ring_separable.values.reshape(-1, 1))\n",
    "\n",
    "X_train_ring_merged = scaler_ring.fit_transform(X_A2_ring_merged) # Trainig set 2\n",
    "y_train_ring_merged = scaler_ring.fit_transform(y_A2_ring_merged.values.reshape(-1, 1))\n",
    "\n",
    "X_test_ring_normalized = scaler_ring.fit_transform(X_A2_ring_test) # Test\n",
    "y_test_ring_normalized = scaler_ring.fit_transform(y_A2_ring_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2-bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "\n",
    "# Before normalizing the second dataset, it needs to be treated.\n",
    "# We have to treat the categorical data and the \"unknown\" values.\n",
    "\n",
    "# Treatment of the categorical data and the \"unknown\" values\n",
    "# Replace \"unknown\" values with NaN\n",
    "df_bank_additional.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# Handling missing values\n",
    "df_bank_additional.fillna(df_bank_additional.mode().iloc[0], inplace=True)\n",
    "\n",
    "# Extract the column names from the first row\n",
    "df_bank_additional.columns = df_bank_additional.iloc[0]\n",
    "df_bank_additional = df_bank_additional[1:]\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df_bank_additional.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_bank_additional[column] = label_encoder.fit_transform(df_bank_additional[column])\n",
    "\n",
    "# Separate the target variable\n",
    "X_bank_additional = df_bank_additional.drop(\"y\", axis=1)  # Features\n",
    "y_bank_additional = df_bank_additional[\"y\"]  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "# Now, we normalize the data\n",
    "scaler_bank = MinMaxScaler()\n",
    "X_train_bank_additional = scaler_bank.fit_transform(X_bank_additional) # bank additional\n",
    "# Reshape the array to a 2D shape (required by MinMaxScaler)\n",
    "y_train_bank_additional = scaler_bank.fit_transform(y_bank_additional.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2-bank\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 80% for training\n",
    "# Extract the remaining 20% for testing\n",
    "# Splitting A2-bank dataset\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = train_test_split(\n",
    "    X_train_bank_additional, y_train_bank_additional, test_size=0.20, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liver disorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 4\n"
     ]
    }
   ],
   "source": [
    "# Liver disorder\n",
    "# We check if this dataset have missing values:\n",
    "missing_values_count = df_liver_Disorder.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the categorical data into numerical\n",
    "# Observing the dataset, there is a column with categorical data, which is the \"Gender\" with the name 1\n",
    "gender_mapping = {'Female': 0, 'Male': 1}\n",
    "df_liver_Disorder[1] = df_liver_Disorder[1].replace(gender_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We handle missing values using median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_liver_Disorder_imputed = pd.DataFrame(imputer.fit_transform(df_liver_Disorder), columns=df_liver_Disorder.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = df_liver_Disorder_imputed.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle outliers using IQR method\n",
    "def handle_outliers_iqr(data, threshold=1.5):\n",
    "    data_copy = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    Q1 = data_copy.quantile(0.25)\n",
    "    Q3 = data_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    data_copy[(data_copy < lower_bound) | (data_copy > upper_bound)] = np.nan\n",
    "\n",
    "    # Handle missing values using median imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data_imputed = imputer.fit_transform(data_copy)\n",
    "    \n",
    "    # Convert back to DataFrame with original column names\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "    return data_imputed\n",
    "\n",
    "# Handle outliers in all feature variables (columns) of df_liver_Disorder\n",
    "df_liver_Disorder_no_outliers = handle_outliers_iqr(df_liver_Disorder_imputed)\n",
    "\n",
    "# Shuffle\n",
    "df_liver_Disorder_shuffled = df_liver_Disorder_no_outliers.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Liver disorder dataset: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = df_liver_Disorder_shuffled.isnull().sum().sum()\n",
    "print(f\"Number of missing values in Liver disorder dataset: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_liver_Disorder = df_liver_Disorder_shuffled.iloc[:, :-1]\n",
    "y_liver_Disorder = df_liver_Disorder_shuffled.iloc[:, -1]\n",
    "\n",
    "# Normalize input variables\n",
    "scaler_liver_Disorder = MinMaxScaler()\n",
    "X_liver_Disorder_normalized_no_outliers = scaler_liver_Disorder.fit_transform(X_liver_Disorder)\n",
    "y_liver_Disorder_normalized_no_outliers = scaler_liver_Disorder.fit_transform(y_liver_Disorder.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third dataset, Liver disorder\n",
    "# Split the data into validation-training and testing sets\n",
    "# Extract the first 80% for training\n",
    "# Extract the remaining 20% for testing\n",
    "# Splitting liver_Disorder dataset\n",
    "X_train_wineQuality, X_test_wineQuality, y_train_wineQuality, y_test_wineQuality = train_test_split(\n",
    "    X_liver_Disorder_normalized_no_outliers,\n",
    "    y_liver_Disorder_normalized_no_outliers,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform supervised training of 3 classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (support vector machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of SVM: kernel, and parameters\n",
    "# Create an SVM classifier with the desired parameters\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_classifier, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "expected_error = 1 - cv_scores.mean()  # Expected classification error\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Expected classification error from cross-validation:\", expected_error)\n",
    "print(\"Classification error on the test set:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP (back propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of BP: architecture of the network, learning rate and momentum, activation function, and number of epochs\n",
    "\n",
    "# Define the function for custom scoring (classification error)\n",
    "def classification_error(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Define the optimizer with custom learning rate and momentum\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_turbine, y_train_turbine, cv=5, scoring=make_scorer(classification_error))\n",
    "expected_error = 1 - cv_scores.mean()  # Expected classification error\n",
    "\n",
    "# Fit the model to the entire training set\n",
    "model.fit(X_train_turbine, y_train_turbine, epochs=1000, batch_size=32, verbose=2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_turbine)\n",
    "\n",
    "# Calculate the classification error on the test set\n",
    "test_error = classification_error(y_test, y_pred)\n",
    "\n",
    "print(\"Expected classification error from cross-validation:\", expected_error)\n",
    "print(\"Classification error on the test set:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR (multi-linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression classifier with the desired parameters\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logistic_regression, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "expected_error = 1 - cv_scores.mean()  # Expected classification error\n",
    "\n",
    "# Train the logistic regression classifier on the entire training set\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Expected classification error from cross-validation:\", expected_error)\n",
    "print(\"Classification error on the test set:\", test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
